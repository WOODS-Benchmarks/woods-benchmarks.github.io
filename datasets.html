<!doctype html>
<html lang="en">

<head>
    <!-- Required meta tags -->
    <meta charset="utf-8">
    <meta name="viewport"
        content="width=device-width, initial-scale=1, shrink-to-fit=no">

    <link rel="icon" href="/assets/logo.png" type="image/png">
    <title>WOODS</title>
    <!-- Bootstrap CSS -->
    <link rel="stylesheet" href="css/bootstrap.css">
    <link rel="stylesheet"
        href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
    <script
        src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script
        src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>
    <!-- main css -->
    <link rel="stylesheet" href="css/style.css">
    <script
        src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.3/jquery.min.js"></script>

</head>

<body data-spy="scroll" data-target="#sidebar" data-offset="200">

    <!--================Header Menu Area =================-->
    <div class="header">
        <a class="logo" href="index.html"><img src="/assets/banner_white.png"
                width="175px" alt=""></a>
        <div class="header-right">
            <a class="nav-link" href="index.html">Home</a></li>
            <a class="nav-link"
                href="https://woods.readthedocs.io/en/latest/installation.html">Get
                Started</a>
            <a class="nav-link" href="datasets.html">Datasets</a>
            <a class="nav-link" href="leaderboard.html">Leaderboard</a>
            <a class="nav-link" href="updates.html">Updates</a>
            <a class="nav-link"
                href="https://woods.readthedocs.io/en/latest">Read the Docs</a>

        </div>
    </div>
    <!--================Work Area =================-->
    <!-- Top Navbar -->
    <div class="row MainContent">
        <!--Side Nav Bar -->
        <nav class="col-xs-3 bs-docs-sidebar">
            <ul id="sidebar" class="nav flex-column fixed">
                <li>
                    <a class="nav-link" href="#overview"><b>Overview</b></a>
                </li>
                <li>
                    <a class="nav-link" href="#spuriousfourier"><b>Spurious
                        Fourier</b>: Spurious features encoded in the frequency domain</a>
                    <ul class="nav flex-column">
                        <li><a class="nav-link ml-3 my-1"
                                href="#SF-motivation">Motivation</a></li>
                        <li><a class="nav-link ml-3 my-1"
                                href="#SF-problem">Problem</a></li>
                    </ul>
                </li>
                <li>
                    <a class="nav-link" href="#tcmnist"><b>TCMNIST</b>: A study of domain definitions in sequential data</a>
                    <ul class="nav flex-column">
                        <li><a class="nav-link ml-3 my-1"
                                href="#tcmnist-motivation">Motivation</a></li>
                        <li><a class="nav-link ml-3 my-1"
                                href="#tcmnist-source">TCMNIST-Source</a></li>
                        <li><a class="nav-link ml-3 my-1"
                                href="#tcmnist-time">TCMNIST-Time</a></li>
                    </ul>
                </li>
                <li>
                    <a class="nav-link" href="#CAP"><b>CAP</b>: Sleep classification across different machines</a>
                    <ul class="nav flex-column">
                        <li><a class="nav-link ml-3 my-1"
                                href="#CAP-motivation">Motivation</a></li>
                        <li><a class="nav-link ml-3 my-1"
                                href="#CAP-problem">Problem</a></li>
                        <li><a class="nav-link ml-3 my-1"
                                href="#CAP-download">Download</a></li>
                        <li><a class="nav-link ml-3 my-1"
                                href="#CAP-source-citation">Source &
                                Citation</a></li>
                        <li><a class="nav-link ml-3 my-1"
                                href="#CAP-license">License</a></li>
                    </ul>
                </li>
                <li>
                    <a class="nav-link" href="#SEDFx"><b>SEDFx</b>: Sleep classification across age groups</a>
                    <ul class="nav flex-column">
                        <li><a class="nav-link ml-3 my-1"
                                href="#SEDFx-motivation">Motivation</a></li>
                        <li><a class="nav-link ml-3 my-1"
                                href="#SEDFx-problem">Problem</a></li>
                        <li><a class="nav-link ml-3 my-1"
                                href="#SEDFx-download">Download</a></li>
                        <li><a class="nav-link ml-3 my-1"
                                href="#SEDFx-source-citation">Source &
                                Citation</a></li>
                        <li><a class="nav-link ml-3 my-1"
                                href="#SEDFx-license">License</a></li>
                    </ul>
                </li>
                <li>
                    <a class="nav-link" href="#PCL"><b>PCL</b>: Motor imagery classification across data-gathering procedures</a>
                    <ul class="nav flex-column">
                        <li><a class="nav-link ml-3 my-1"
                                href="#PCL-motivation">Motivation</a></li>
                        <li><a class="nav-link ml-3 my-1"
                                href="#PCL-problem">Problem</a></li>
                        <li><a class="nav-link ml-3 my-1"
                                href="#PCL-download">Download</a></li>
                        <li><a class="nav-link ml-3 my-1"
                                href="#PCL-source-citation">Source &
                                Citation</a></li>
                        <li><a class="nav-link ml-3 my-1"
                                href="#PCL-license">License</a></li>
                    </ul>
                </li>
                <li>
                    <a class="nav-link" href="#LSA64"><b>LSA64</b>: Sign language video classification across speakers</a>
                    <ul class="nav flex-column">
                        <li><a class="nav-link ml-3 my-1"
                                href="#LSA64-motivation">Motivation</a></li>
                        <li><a class="nav-link ml-3 my-1"
                                href="#LSA64-problem">Problem</a></li>
                        <li><a class="nav-link ml-3 my-1"
                                href="#LSA64-download">Download</a></li>
                        <li><a class="nav-link ml-3 my-1"
                                href="#LSA64-source-citation">Source &
                                Citation</a></li>
                        <li><a class="nav-link ml-3 my-1"
                                href="#LSA64-license">License</a></li>
                    </ul>
                </li>
                <li>
                    <a class="nav-link" href="#HHAR"><b>HHAR</b>: Human activity recognition across smart devices</a>
                    <ul class="nav flex-column">
                        <li><a class="nav-link ml-3 my-1"
                                href="#HHAR-motivation">Motivation</a></li>
                        <li><a class="nav-link ml-3 my-1"
                                href="#HHAR-problem">Problem</a></li>
                        <li><a class="nav-link ml-3 my-1"
                                href="#HHAR-download">Download</a></li>
                        <li><a class="nav-link ml-3 my-1"
                                href="#HHAR-source-citation">Source &
                                Citation</a></li>
                        <li><a class="nav-link ml-3 my-1"
                                href="#HHAR-license">License</a></li>
                    </ul>
                </li>
                <li>
                    <a class="nav-link" href="#AusElec"><b>AusElec</b>: Forecasting of energy consumption throughout the year</a>
                    <ul class="nav flex-column">
                        <li><a class="nav-link ml-3 my-1"
                                href="#AusElec-motivation">Motivation</a></li>
                        <li><a class="nav-link ml-3 my-1"
                                href="#AusElec-problem">Problem</a></li>
                        <li><a class="nav-link ml-3 my-1"
                                href="#AusElec-download">Download</a></li>
                        <li><a class="nav-link ml-3 my-1"
                                href="#AusElec-source-citation">Source &
                                Citation</a></li>
                        <li><a class="nav-link ml-3 my-1"
                                href="#AusElec-license">License</a></li>
                    </ul>
                </li>
                <li>
                    <a class="nav-link" href="#IEMOCAP"><b>IEMOCAP</b>: Emotion recognition across emotion shifts</a>
                    <ul class="nav flex-column">
                        <li><a class="nav-link ml-3 my-1"
                                href="#IEMOCAP-motivation">Motivation</a></li>
                        <li><a class="nav-link ml-3 my-1"
                                href="#IEMOCAP-problem">Problem</a></li>
                        <li><a class="nav-link ml-3 my-1"
                                href="#IEMOCAP-download">Download</a></li>
                        <li><a class="nav-link ml-3 my-1"
                                href="#IEMOCAP-source-citation">Source &
                                Citation</a></li>
                        <li><a class="nav-link ml-3 my-1"
                                href="#IEMOCAP-license">License</a></li>
                    </ul>
                </li>
            </ul>
        </nav>
        <div class="col-md-6">
            <div class="content"
                style="padding-top: -1000px; padding-left: 5em ">
                <h1 class="list-item" id="overview">Overview</h1>
                <div class="img-with-text">
                    <img src="assets/Dataset_summary.png" alt="woods-datasets"
                        style="width: 100%">
                    <p style="line-height: 1.15; margin-top: 0.5cm;">Figure 1: Summary of WOODS benchmark: tasks, modalities, domains and distribution shifts.</p>
                </div>
                <hr />
                <h1 class="list-item" id="spuriousfourier">Spurious Fourier</h1>
                <h2 class="list-item" id="SF-motivation">Motivation</h2>
                <p>
                    Consider the task of distinguishing cows and camels in
                    pictures. We train a deep learning model to minimize the
                    empirical risk on a labeled dataset. However, the dataset is
                    heavily tainted by selection bias, as the vast majority of
                    cow images were taken in green pastures, and the vast
                    majority of camel images were taken in sandy areas. At test
                    time, we observe that our deep learning model consistently
                    and confidently classifies images of cows on sandy beaches
                    as camels. We conclude that the model successfully minimized
                    the empirical risk over the training dataset by leveraging
                    the selection bias, thus classifying green backgrounds as
                    cows and beige backgrounds as camels. We propose the Spurious-Fourier dataset as an adaptation of the cow or camel classification problem to time series.
                </p>
                <h2 class="list-item" id="SF-problem">Problem</h2>
                <div class="img-with-text">
                    <img src="assets/Spurious_Fourier_appendix.png"
                        alt="spuriousfourier-problem" class="task">
                    <p style="line-height: 1.15; margin-top: 0.5cm;">Figure 2: Description of the Spurious-Fourier dataset. Signals have one low-frequency peak and one high-frequency peak. They are then constructed from the Fourier spectrum with an inverse Fourier transform. (b) Examples of reconstructed signals, both signals have the same high frequency, but different low frequencies, which are hard to distinguish visually. </p>
                </div>
                <p>
                    We create a dataset composed of one-dimensional signals, where the task is to perform binary classification based on the frequency characteristics. Signals are constructed from Fourier spectra with one low-frequency peak (L<sub>A</sub>=2Hz or L<sub>B</sub>=4Hz) and one high-frequency peak (H<sub>A</sub>=7Hz or H<sub>B</sub>=9Hz), see Figure 2. Domains D<sup>d</sup>|<sub>d in {10%, 80%, 90%}</sub> contain signal-label pairs, where the label is a noisy function of the low- and high-frequencies such that low-frequency peaks bear a varying correlation of d with the label and high-frequency peaks bear an invariant correlation of 75% with the label.
                </p>
                <hr />
                <h1 class="list-item" id="tcmnist">Temporal Colored MNIST</h1>
                <h2 class="list-item" id="tcmnist-motivation">Motivation</h2>
                <p>
                    In light of the previously mentioned cow or camel
                    classification problem, Arjovsky et al. (2020) proposed the
                    CMNIST dataset as a synthetic investigation of this problem.
                    We extend this widely used dataset to time
                    series, where we explore new domain formulations in time series.
                </p>
                <h3 class="list-item" id="tcmnist-source">Temporal Colored MNIST
                    with source domains
                </h3>
                <div class="img-with-text">
                    <img src="assets/TCMNIST_Source.png"
                        alt="tcmnist-source" class="task">
                    <p style="line-height: 1.15; margin-top: 0.5cm;">Figure 3: Description of the Temporal Colored MNIST dataset with source domains. (a) Data samples are videos of four colored MNIST digits where the task is to predict whether the sum of the current and previous digits in the sequence is odd or even. (b) Spuriously correlated color is added to each digit such that the correlation is constant among the frames of a video, but varies between video from different domains d in {10%, 80%, 90%}.</p>
                </div>
                <p>
                    In Temporal Colored MNIST with source domains (TCMNIST-Source), we create a binary classification task of video frames. Videos are sequences of four colored MNIST digits where the goal is to predict whether the sum of the current and previous digits in the sequence is odd or even, see Figure 3(a). Prediction is made for all frames except for the first one. The label is a noisy function of the digit and color, such that the color bears a varying correlation of d with the label of the frame, and the digit sums bears an invariant correlation of 75% with the label of the frame. Domains are created such that the color correlation is constant among the frames of a video, but varies between video from different domains d in {10%, 80%, 90%}. The domain definition is depicted in Figure 3(b).
                </p>
                <h3 class="list-item" id="tcmnist-time">Temporal Colored MNIST with time domains</h3>                <div class="img-with-text">
                    <img src="assets/TCMNIST_Time.png"
                        alt="tcmnist-source" class="task">
                    <p style="line-height: 1.15; margin-top: 0.5cm;">Figure 4: Description of the Temporal Colored MNIST dataset with time domains. (a) Data samples are videos of four colored MNIST digits where the task is to predict whether the sum of the current and previous digits in the sequence is odd or even. (b) Spuriously correlated color is added to each digit such that the correlation varies across frames. However, videos all have the same sequence of color correlation, where the first labeled frame correlation is 90%, second is 80% and third is 10%.</p>
                </div>
                <p>
                    In Temporal Colored MNIST with time domains (TCMNIST-Time), we create a binary classification task of video frames. Videos are sequences of four colored MNIST digits where the goal is to predict whether the sum of the current and previous digits in the sequence is odd or even, see Figure 4(a). Prediction is made for all frames except for the first one. The label is a noisy function of the digit and color, such that the color bears a varying correlation of d with the label of the frame, and the digit sums bears an invariant correlation of 75% with the label of the frame. Domains are created such that the color correlation varies across frames. However, videos all have the same sequence of color correlation, where the first labeled frame correlation is 90%, second is 80% and third is 10%. The domain definition is depicted in Figure 4(b).
                </p>
                <hr />
                <h1 class="list-item" id="CAP">CAP</h1>
                <h2 class="list-item" id="CAP-motivation">Motivation</h2>
                <p>
                    A recurrent problem in computational medicine is that models trained on data from a given recording device will not generalize to data coming from another device, even when both devices are from a similar equipment provider. Failure to generalize to unseen machines can cause critical issues for clinical practice because a false sense of confidence in a model could lead to a false diagnosis. We study these machinery-induced distribution shifts with the CAP dataset.
                </p>
                <h2 class="list-item" id="CAP-problem">Problem</h2>
                <div class="img-with-text">
                    <img src="assets/CAP.png" alt="CAP-problem"
                        class="task">
                    <p style="line-height: 1.15; margin-top: 0.5cm;">Figure 5: Summary of the CAP dataset. (a) The task is to perform sleep stage classification from EEG measurements. (b) The dataset has five source domains, where each domain contains data gathered with a different machine. The goal is to generalize to unseen machines.</p>
                </div>
                <p>
                    We consider the sleep stage classification task from electroencephalographic (EEG) measurements. The dataset has five source domains, where each domain contains data gathered with a different machine. The goal is to generalize to unseen machines.
                </p>
                <h2 class="list-item" id="CAP-download">Download</h2>
                <p>
                    The preprocessed data is available on <a href="https://academictorrents.com/details/500d0c473108ef72e01b0f8037251b09331467f9">Academic Torrents</a> and via <a href="https://drive.google.com/uc?id=1NFwX2CqLrenWF4az0c6J-OglAoD48PAT">Google Drive</a>. It can also be directly downloaded through the <a href="https://github.com/jc-audet/WOODS">WOODS repository</a>.
                </p>
                <h2 class="list-item" id="CAP-source-citation">Source & Citation
                </h2>
                <p class="Citation-box">
                    [1] Terzano MG, Parrino L, Sherieri A, Chervin R,
                    Chokroverty S, Guilleminault C, Hirshkowitz M, Mahowald M,
                    Moldofsky H, Rosa A, Thomas R, Walters A. Atlas, rules, and
                    recording techniques for the scoring of cyclic alternating
                    pattern (CAP) in human sleep. Sleep Med. 2001
                    Nov;2(6):537-53. doi: 10.1016/s1389-9457(01)00149-6. Erratum
                    in: Sleep Med. 2002 Mar;3(2):185. PMID: 14592270.
                    <br />
                    <br />

                    [2] Goldberger, A., Amaral, L., Glass, L., Hausdorff, J.,
                    Ivanov, P. C., Mark, R., ... & Stanley, H. E. (2000).
                    PhysioBank, PhysioToolkit, and PhysioNet: Components of a
                    new research resource for complex physiologic signals.
                    Circulation [Online]. 101 (23), pp. e215–e220.
                </p>
                <h2 class="list-item" id="CAP-license">License</h2>
                <p>
                    This project is licensed under the <a
                        href="https://opendatacommons.org/licenses/by/summary/index.html">
                        Open Data Commons Attribution license v1.0. </a>
                </p>
                <hr />
                <h1 class="list-item" id="SEDFx">SEDFx</h1>
                <h2 class="list-item" id="SEDFx-motivation">Motivation</h2>
                <p>
                    In clinical settings, we train a model on the data gathered from a limited number of patients and hope this model will generalize to new patients in the future. However, this generalization between observed patients in the training dataset and new patients is not guaranteed. Distribution shifts caused by shifts in patient demographics (e.g., age, gender, and ethnicity) can cause the model to fail. We study age demographic shift with the SEDFx dataset. 
                </p>
                <h2 class="list-item" id="SEDFx-problem">Problem</h2>
                <div class="img-with-text">
                    <img src="assets/SEDFx.png" alt="SEDFx-problem"
                        class="task">
                    <p style="line-height: 1.15; margin-top: 0.5cm;">Figure 6: Summary of the SEDFx dataset. (a) The task is to perform sleep stage classification from EEG measurements. (b) The dataset has four source domains, where each domain contains data from participants of a certain age group. The goal is to generalize to unseen age groups.</p>
                </div>
                <p>
                    We consider the sleep classification task from EEG measurements. The dataset has four source domains, where each domain contains data from participants of a certain age group. The goal is to generalize to an unseen age demographic.
                </p>
                <h2 class="list-item" id="SEDFx-download">Download</h2>
                <p>
                    The preprocessed data is available on <a href="https://academictorrents.com/details/58ea303dce39ffe822bec7704f9eb65e4173defd">Academic Torrents</a> and via <a href="https://drive.google.com/uc?id=15j_bsiOmMJb42mG712Vhv3jZ4MQSOgoT">Google Drive</a>. It can also be directly downloaded through the <a href="https://github.com/jc-audet/WOODS">WOODS repository</a>.
                </p>
                <h2 class="list-item" id="SEDFx-source-citation">Source &
                    Citation</h2>
                <p class="Citation-box">
                    [1] B. Kemp, A. H. Zwinderman, B. Tuk, H. A. C. Kamphuisen
                    and J. J. L. Oberye, "Analysis of a sleep-dependent neuronal
                    feedback loop: the slow-wave microcontinuity of the EEG," in
                    IEEE Transactions on Biomedical Engineering, vol. 47, no. 9,
                    pp. 1185-1194, Sept. 2000, doi: 10.1109/10.867928.
                    <br />
                    <br />

                    [2] Goldberger, A., Amaral, L., Glass, L., Hausdorff, J.,
                    Ivanov, P. C., Mark, R., ... & Stanley, H. E. (2000).
                    PhysioBank, PhysioToolkit, and PhysioNet: Components of a
                    new research resource for complex physiologic signals.
                    Circulation [Online]. 101 (23), pp. e215–e220.
                </p>
                <h2 class="list-item" id="SEDFx-license">License</h2>
                <p>
                    This project is licensed under the <a
                        href="https://opendatacommons.org/licenses/by/summary/index.html">
                        Open Data Commons Attribution license v1.0. </a>
                </p>
                <hr />
                <h1 class="list-item" id="PCL">PCL</h1>
                <h2 class="list-item" id="PCL-motivation">Motivation</h2>
                <p>
                    Aside from changes in the recording device and shifts in patient demographics, human intervention in the data gathering process is another contributing factor to the distribution shift that can lead to failure of clinical models (e.g., Camelyon17). This challenge is especially prevalent in temporal medical data (e.g., EEG, MEG, and others) because recording devices are complex tools greatly affected by nonlinear effects and modulations. These effects are often caused by context and preparations made before the recording. We study these procedural shifts with the PCL dataset.
                </p>
                <h2 class="list-item" id="PCL-problem">Problem</h2>
                <div class="img-with-text">
                    <img src="assets/PCL.png" alt="PCL-problem"
                        class="task">
                    <p style="line-height: 1.15; margin-top: 0.5cm;">Figure 7: Summary of the PCL dataset. (a) The task is to perform motor imagery classification from EEG measurements. (b) The dataset has three source domains, where each domain contains a dataset from a different research group carrying out the same task. The goal is to generalize to unseen datasets of the same task.</p>
                </div>
                <p>
                    We consider the motor imagery task from electroencephalographic (EEG) measurements. The dataset has three source domains, where each domain contains a dataset from a different research group carrying out the same task. The goal is to generalize to unseen data gathering processes.
                </p>
                <h2 class="list-item" id="PCL-download">Download</h2>
                <p>
                    The preprocessed data is available on <a href="https://academictorrents.com/details/e8b0a24177988f9c3f8c3c63a8212546f67a25a3">Academic Torrents</a> and via <a href="https://drive.google.com/uc?id=118DNxHpzeJwVTM22wzZhSiOuDsno0nay">Google Drive</a>. It can also be directly downloaded through the <a href="https://github.com/jc-audet/WOODS">WOODS repository</a>.
                </p>
                <h2 class="list-item" id="PCL-source-citation">Source & Citation
                </h2>
                <p class="Citation-box">
                    [1] Tangermann, M., Müller, K.R., Aertsen, A., Birbaumer,
                    N., Braun, C., Brunner, C., Leeb, R., Mehring, C., Miller,
                    K.J., Mueller-Putz, G. and Nolte, G., 2012. Review of the
                    BCI competition IV. Frontiers in neuroscience, 6, p.55.
                    <br />
                    <br />
                    [2] Schalk, G., McFarland, D.J., Hinterberger, T.,
                    Birbaumer, N. and Wolpaw, J.R., 2004. BCI2000: a
                    general-purpose brain-computer interface (BCI) system. IEEE
                    Transactions on biomedical engineering, 51(6), pp.1034-1043.
                    <br />
                    <br />
                    [3] Goldberger, A.L., Amaral, L.A., Glass, L., Hausdorff,
                    J.M., Ivanov, P.C., Mark, R.G., Mietus, J.E., Moody, G.B.,
                    Peng, C.K., Stanley, H.E. and PhysioBank, P., PhysioNet:
                    components of a new research resource for complex
                    physiologic signals Circulation 2000 Volume 101 Issue 23 pp.
                    E215–E220.
                    <br />
                    <br />
                    [4] Lee MH, Kwon OY, Kim YJ, Kim HK, Lee YE, Williamson J,
                    Fazli S, Lee SW. EEG dataset and OpenBMI toolbox for three
                    BCI paradigms: an investigation into BCI illiteracy.
                    Gigascience. 2019 May 1;8(5):giz002. doi:
                    10.1093/gigascience/giz002. PMID: 30698704; PMCID:
                    PMC6501944.
                </p>
                <h2 class="list-item" id="PCL-license">License</h2>
                <p>
                    This dataset gathers data from multiple sources, therefore
                    has multiple licenses attached to the dataset. The
                    physionetMI dataset is licensed under the <a
                        href="https://opendatacommons.org/licenses/by/summary/index.html">
                        Open Data Commons Attribution license v1.0.</a> The
                    Lee2019_MI dataset is licensed under <a
                        href="https://creativecommons.org/share-your-work/public-domain/cc0">
                        No Rights Reserved License.</a> The BNCI2014001 dataset
                    is freely available on the <a
                        href="http://www.bbci.de/competition/iv/"> BCI
                        Competition IV website</a> with the only restriction
                    that [1] is referenced upon publication of results.
                </p>
                <hr />
                <h1 class="list-item" id="LSA64">LSA64</h1>
                <h2 class="list-item" id="LSA64-motivation">Motivation</h2>
                <div class="img-with-text">
                    <img src="assets/LSA64.png" alt="LSA64-tast"
                        class="task">
                    <p style="line-height: 1.15; margin-top: 0.5cm;">Figure 8: Summary of the LSA64 dataset. (a) The task is to perform signed word classification from videos. (b) The dataset has five source domains, where each domain contains videos of different signers. The goal is to generalize to unseen signers.</p>
                </div>
                <p>
                    Communication is an individualistic way to convey information through different media: text, speech, body language, and many others. However, some media are more distinctive and challenging than others. For example, text communication has less inter-individual variability than body language or speech. If deep learning systems hope to interact with humans effectively, models need to generalize to new and evolving mannerisms, accents, and other subtle variations in communication that significantly impact the meaning of the message conveyed. We study the ability of models to recognize information coming from unseen individuals with the LSA64 dataset.
                </p>
                <h2 class="list-item" id="LSA64-problem">Problem</h2>
                <p>
                    We consider the video classification of signed words in Argentinian Sign Language. The dataset has five source domains, where each domain contains videos of different signers. The goal is to generalize to unseen signers.
                </p>
                <h2 class="list-item" id="LSA64-download">Download</h2>
                <p>
                    The preprocessed data is available on <a href="https://academictorrents.com/details/704bf5981eb337cae7cb518c3abb9d7b6bdf3e49">Academic Torrents</a> and via <a href="https://drive.google.com/uc?id=1YwwSg8Dt178ySp3ht_BLJwl5j5s_IU1m">Google Drive</a>. It can also be directly downloaded through the <a href="https://github.com/jc-audet/WOODS">WOODS repository</a>.
                </p>
                <h2 class="list-item" id="LSA64-source-citation">Source &
                    Citation</h2>
                <p class="Citation-box">
                    [1] Ronchetti, F., Quiroga, F., Estrebou, C., Lanzarini, L.,
                    & Rosete, A. (2016). LSA64: A Dataset of Argentinian Sign
                    Language. XX II Congreso Argentino de Ciencias de la
                    Computación (CACIC).
                </p>
                <h2 class="list-item" id="LSA64-license">License</h2>
                <p>
                    This project is licensed under the <a
                        href="https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode">
                        Creative Commons Attribution-NonCommercial-ShareAlike
                        4.0 International License </a>
                </p>
                <hr />
                <h1 class="list-item" id="HHAR">HHAR</h1>
                <h2 class="list-item" id="HHAR-motivation">Motivation</h2>
                <p>
                    The intrinsic biases from inaccurate and poorly calibrated sensors of smart devices, along with the accumulated biases from everyday use makes human activity recognition a notoriously difficult task when task when done across devices. Contrary to static tasks where uninformative features can often be segmented out from the input features (e.g., background when classifying an animal from an image), invariant features in time series are often highly convoluted with other spurious features. We study the ability of models to ignore spurious information from complex signals with the HHAR dataset.

                </p>
                <h2 class="list-item" id="HHAR-problem">Problem</h2>
                <div class="img-with-text">
                    <img src="assets/HHAR.png" alt="HHAR-problem"
                        class="task">
                    <p style="line-height: 1.15; margin-top: 0.5cm;">Figure 9: Summary of the HHAR dataset. (a) The task is to perform human activity classification from smart devices sensory data. (b) The dataset has five source domains, where each domain contains data gathered with a different smart device. The goal is to generalize to unseen smart devices.</p>
                </div>
                <p>
                    We consider the human activity classification task from accelerometer and gyroscope measurements of smartphones and smartwatches. The dataset has five source domains, where each domain contains data gathered with a different device. The goal is to generalize to unseen smart devices.
                </p>
                <h2 class="list-item" id="HHAR-download">Download</h2>
                <p>
                    The preprocessed data is available on <a href="https://academictorrents.com/details/f48f38de06b3cd560fb90307b5a1997a12bcc29c">Academic Torrents</a> and via <a href="https://drive.google.com/uc?id=1Z3IcrCE-o77p4YrvkCy-Y-0CgCyxVHet">Google Drive</a>. It can also be directly downloaded through the <a href="https://github.com/jc-audet/WOODS">WOODS repository</a>.
                </p>
                <h2 class="list-item" id="HHAR-source-citation">Source &
                    Citation</h2>
                <p class="Citation-box">
                    [1] Allan Stisen, Henrik Blunck, Sourav Bhattacharya, Thor
                    Siiger Prentow, Mikkel Baun Kjærgaard, Anind Dey, Tobias
                    Sonne, and Mads Møller Jensen "Smart Devices are Different:
                    Assessing and Mitigating Mobile Sensing Heterogeneities for
                    Activity Recognition" In Proc. 13th ACM Conference on
                    Embedded Networked Sensor Systems (SenSys 2015), Seoul,
                    Korea, 2015.
                    <br />
                    <br />

                    [2] Dua, D. and Graff, C. (2019). UCI Machine Learning
                    Repository [http://archive.ics.uci.edu/ml]. Irvine, CA:
                    University of California, School of Information and Computer
                    Science.
                </p>
                <h2 class="list-item" id="HHAR-license">License</h2>
                <p>
                    This project is licensed under the <a
                        href="https://opendatacommons.org/licenses/by/summary/index.html">
                        Open Data Commons Attribution license v1.0. </a>
                </p>
                <hr />
                <h1 class="list-item" id="AusElec">AusElec</h1>
                <h2 class="list-item" id="AusElec-motivation">Motivation</h2>
                <p>
                    Seasonality is the property of time series where recurring characteristics appear every cycle of a fixed period, e.g., weekly. A common practice in the forecasting field is to provide models with additional information, e.g., day of week in order to allow models to leverage seasonality for better predictions. However, holidays is a seasonality of time series that is very sparse which models often fail to capture. We study the performance of models on sparse seasonality with the AusElec dataset.
                </p>
                <h2 class="list-item" id="AusElec-problem">Problem</h2>
                <div class="img-with-text">
                    <img src="assets/AusElec.png" alt="AusElec-problem"
                        class="task">
                    <p style="line-height: 1.15; margin-top: 0.5cm;">Figure 10: Summary of the AusElec dataset. (a) The task is to forecast electricity consumption. (b) The dataset has 13 time domains, where each domain contains data from different months and holidays. The goal is to perform well on all seasonalities.</p>
                </div>
                <p>
                    We consider the electricity consumption forecasting task. The dataset has 13 time domains, where each domain contains data from different months and holidays. The goal is to perform well on all seasonalities. 
                </p>
                <h2 class="list-item" id="AusElec-download">Download</h2>
                <p>
                    The preprocessed data is available through Huggingface <a href="https://huggingface.co/datasets/monash_tsf">monash_tsf</a> dataset and through <a href="https://zenodo.org/record/4659727#.Yquiu9LMJhE">Zenodo</a>. It can also be directly downloaded through the <a href="https://github.com/jc-audet/WOODS">WOODS repository</a>.
                </p>
                <h2 class="list-item" id="AusElec-source-citation">Source &
                    Citation</h2>
                <p class="Citation-box">
                    [1] Robin John Hyndman and George Athanasopoulos. Forecasting: Principles and Practice.
                    OTexts, Australia, 2nd edition, 2018.
                    <br />
                    <br />

                    [2] Rakshitha Godahewa, Christoph Bergmeir, Geoffrey I Webb, Rob J Hyndman, and Pablo
                    Montero-Manso. Monash time series forecasting archive. arXiv preprint arXiv:2105.06643,
                    2021.
                    
                </p>
                <h2 class="list-item" id="AusElec-license">License</h2>
                <p>
                    This dataset is licensed under the <a href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution 4.0 International License</a>. 
                </p>


                <hr />
                <h1 class="list-item" id="IEMOCAP">IEMOCAP</h1>
                <h2 class="list-item" id="IEMOCAP-motivation">Motivation</h2>
                <p>
                    Speakers tend to maintain an emotional state over a conversation. However, external stimuli can invoke a shift in the emotional state of speakers. Such emotion shift are often sparsely represented in the data, making it hard for models to classify them adequately. Recent work on emotion recognition models show the failure of existing models to adapt to those emotion shift. We study the performance of models on emotional shift with the IEMOCAP dataset
                </p>
                <h2 class="list-item" id="IEMOCAP-problem">Problem</h2>
                <div class="img-with-text">
                    <img src="assets/IEMOCAP.png" alt="IEMOCAP-problem"
                        class="task">
                    <p style="line-height: 1.15; margin-top: 0.5cm;">Figure 11: Summary of the IEMOCAP dataset. (a) The task is to perform emotion recognition from multi modal data (video, sound, text). (b) The dataset has 11 time domains, where each domain contains data from a different emotion shifts during conversations. The goal is to perform well on all conversational emotion shifts.</p>
                </div>
                <p>
                    We consider the emotion recognition task. The dataset has 11 time domains, where each domain contains data from a different emotion shift during conversations. The goal is to perform well on all conversational emotion shifts.
                </p>
                <h2 class="list-item" id="IEMOCAP-download">Download</h2>
                <p>
                    THe IEMOCAP is only accessible through the <a href="https://sail.usc.edu/iemocap/iemocap_release.htm">IEMOCAP website</a>. You will need to adhere to their license agreement.
                </p>
                <h2 class="list-item" id="IEMOCAP-source-citation">Source &
                    Citation</h2>
                <p class="Citation-box">
                    [1] Murtaza Bulut, Chi-Chun Lee, Abe Kazemzadeh, Emily Mower, Samuel Kim, Jeannette N
                    Chang, Sungbok Lee, and Shrikanth S Narayanan. Iemocap: Interactive emotional dyadic
                    motion capture database. Language resources and evaluation, 42(4):335–359, 2008.
                    
                </p>
                <h2 class="list-item" id="IEMOCAP-license">License</h2>
                <p>
                    This dataset is licensed under the <a href="https://sail.usc.edu/iemocap/iemocap_release.htm">special license</a>. 
                    
                </p>
            </div>
        </div>
    </div>
</body>

</html>
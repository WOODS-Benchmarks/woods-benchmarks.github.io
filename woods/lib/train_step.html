<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>woods.lib.train_step API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>woods.lib.train_step</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import time
import numpy as np

import torch
from torch import nn, optim

from woods.lib import datasets
from woods.lib import models
from woods.lib import objectives
from woods.lib import hyperparams
from woods.lib import utils

## Train function
def train_step(model, loss_fn, objective, dataset, in_loaders_iter, optimizer, device):
    &#34;&#34;&#34; Make a single training step for a model on a dataset of the step setup with an objective

    Args:
        model (nn.Module): Model to train
        loss_fn (fn): pytorch nn loss function
        objective (Objective): Instance of an Objective object from lib.objectives
        dataset (Multi_Domain_Dataset): Instance of a Multi_Domain_Dataset object from lib.datasets
        in_loaders_iter (Iterable): Iterable that generates tuples of dataloaders coming from each environment (usually gotten by a zip() call)
        optimizer (torch.optim): Instance of a torch.optim optimizer
        device (str): device on which the training happens

    Returns:
        nn.Module: returns the model updated from a training step
    &#34;&#34;&#34;
    model.train()

    ts = torch.tensor(dataset.get_pred_time()).to(device)

    # Get next batch of training data
    # TODO: Fix that awful patch with infinite
    try:
        batch_loaders = next(in_loaders_iter)
    except StopIteration:
        _, loaders = dataset.get_train_loaders()
        in_loaders_iter = zip(*loaders)
        batch_loaders = next(in_loaders_iter)

    minibatches_device = [(x, y) for x,y in batch_loaders]

    ## Group all inputs and send to device
    all_x = torch.cat([x for x,y in minibatches_device]).to(device)
    all_y = torch.cat([y for x,y in minibatches_device]).to(device)
    all_out = []

    ## Group all inputs and get prediction
    all_out, pred = model(all_x, ts)

    # Get train loss for train environment
    train_env = [i for i, t in enumerate(ts) if i != len(ts)-1]
    env_losses = torch.zeros(len(train_env))
    for i, e in enumerate(train_env):
        env_out = all_out[e]
        env_loss = loss_fn(env_out, all_y[:,e])  
        env_losses[i] = env_loss
        objective.gather_logits_and_labels(env_out, all_y[:,e])

    # back propagate
    optimizer.zero_grad()
    objective.backward(env_losses)
    optimizer.step()

    return model


def train_step_setup(flags, training_hparams, model, objective, dataset, device):
    &#34;&#34;&#34; Train a model on a dataset of the step setup with an objective

    Args:
        flags (Namespace): training arguments
        training_hparams (dict): training related hyperparameters (lr, weight decay, batchsize, etc.)
        model (nn.Module): Model to be trained
        objective (Objective): Instance of an Objective object from lib.objectives
        dataset (Multi_Domain_Dataset): Instance of a Multi_Domain_Dataset object from lib.datasets
        device (str): device on which the training happens

    Returns:
        dict: records from training of the model
    &#34;&#34;&#34;

    loss_fn = nn.NLLLoss(weight=dataset.get_class_weight().to(device))
    optimizer = optim.Adam(model.parameters(), lr=training_hparams[&#39;lr&#39;], weight_decay=training_hparams[&#39;weight_decay&#39;])
    
    record = {}
    step_times = []
    
    t = utils.setup_pretty_table(flags)

    train_names, train_loaders = dataset.get_train_loaders()
    n_batches = np.sum([len(train_l) for train_l in train_loaders])

    ## Is this in the loop or not?
    train_loaders_iter = zip(*train_loaders)

    for step in range(1, dataset.N_STEPS + 1):

        ## Make training step and report accuracies and losses
        start = time.time()
        model = train_step(model, loss_fn, objective, dataset, train_loaders_iter, optimizer, device)
        step_times.append(time.time() - start)

        if step % dataset.CHECKPOINT_FREQ == 0 or (step-1) == 0:

            val_start = time.time()
            checkpoint_record = get_accuracies_step(model, loss_fn, dataset, device)
            val_time = time.time() - val_start

            record[str(step)] = checkpoint_record

            t.add_row([step] 
                    + [&#34;{:.2f} :: {:.2f}&#34;.format(record[str(step)][str(e)+&#39;_in_acc&#39;], record[str(step)][str(e)+&#39;_out_acc&#39;]) for e in dataset.get_envs()]
                    + [&#34;{:.2f}&#34;.format(np.average([record[str(step)][str(e)+&#39;_loss&#39;] for e in train_names[0]]))] 
                    + [&#34;{:.2f}&#34;.format((step*len(train_loaders)) / n_batches)]
                    + [&#34;{:.2f}&#34;.format(np.mean(step_times))]  
                    + [&#34;{:.2f}&#34;.format(val_time)])
            print(&#34;\n&#34;.join(t.get_string().splitlines()[-2:-1]))

    return model, record

def get_accuracies_step(model, loss_fn, dataset, device):

    # Get loaders and their names
    val_names, val_loaders = dataset.get_val_loaders()
    train_names, train_loaders = dataset.get_train_loaders()
    all_names = val_names + train_names
    all_loaders = val_loaders + train_loaders
    ## Get test accuracy and loss
    record = {}
    for name, loader in zip(all_names, all_loaders):
        accuracies, losses = get_split_accuracy(model, loss_fn, dataset, loader, device)
        for i, e in enumerate(name):
            record.update({ e+&#39;_acc&#39;: accuracies[i],
                            e+&#39;_loss&#39;: losses[i]})
    
    return record

def get_split_accuracy(model, loss_fn, dataset, loader, device):

    model.eval()
    losses = []
    nb_correct = 0
    nb_item = 0

    ts = torch.tensor(dataset.get_pred_time()).to(device)
    with torch.no_grad():

        losses = torch.zeros(ts.shape[0], 0).to(device)
        for i, (data, target) in enumerate(loader):

            data, target = data.to(device), target.to(device)

            out, pred = model(data, ts)

            loss = torch.zeros(ts.shape[0], 1).to(device)
            for i, t in enumerate(ts):     # Only consider labels after the prediction at prediction times
                loss[i] += loss_fn(out[i], target[:,i])

            losses = torch.cat((losses, loss), dim=1)

            nb_correct += torch.sum(pred.eq(target).cpu(), dim=0)
            nb_item += pred.shape[0]
        
    return (nb_correct / nb_item).tolist(), torch.mean(losses, dim=1).tolist()</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="woods.lib.train_step.get_accuracies_step"><code class="name flex">
<span>def <span class="ident">get_accuracies_step</span></span>(<span>model, loss_fn, dataset, device)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_accuracies_step(model, loss_fn, dataset, device):

    # Get loaders and their names
    val_names, val_loaders = dataset.get_val_loaders()
    train_names, train_loaders = dataset.get_train_loaders()
    all_names = val_names + train_names
    all_loaders = val_loaders + train_loaders
    ## Get test accuracy and loss
    record = {}
    for name, loader in zip(all_names, all_loaders):
        accuracies, losses = get_split_accuracy(model, loss_fn, dataset, loader, device)
        for i, e in enumerate(name):
            record.update({ e+&#39;_acc&#39;: accuracies[i],
                            e+&#39;_loss&#39;: losses[i]})
    
    return record</code></pre>
</details>
</dd>
<dt id="woods.lib.train_step.get_split_accuracy"><code class="name flex">
<span>def <span class="ident">get_split_accuracy</span></span>(<span>model, loss_fn, dataset, loader, device)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_split_accuracy(model, loss_fn, dataset, loader, device):

    model.eval()
    losses = []
    nb_correct = 0
    nb_item = 0

    ts = torch.tensor(dataset.get_pred_time()).to(device)
    with torch.no_grad():

        losses = torch.zeros(ts.shape[0], 0).to(device)
        for i, (data, target) in enumerate(loader):

            data, target = data.to(device), target.to(device)

            out, pred = model(data, ts)

            loss = torch.zeros(ts.shape[0], 1).to(device)
            for i, t in enumerate(ts):     # Only consider labels after the prediction at prediction times
                loss[i] += loss_fn(out[i], target[:,i])

            losses = torch.cat((losses, loss), dim=1)

            nb_correct += torch.sum(pred.eq(target).cpu(), dim=0)
            nb_item += pred.shape[0]
        
    return (nb_correct / nb_item).tolist(), torch.mean(losses, dim=1).tolist()</code></pre>
</details>
</dd>
<dt id="woods.lib.train_step.train_step"><code class="name flex">
<span>def <span class="ident">train_step</span></span>(<span>model, loss_fn, objective, dataset, in_loaders_iter, optimizer, device)</span>
</code></dt>
<dd>
<div class="desc"><p>Make a single training step for a model on a dataset of the step setup with an objective</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>model</code></strong> :&ensp;<code>nn.Module</code></dt>
<dd>Model to train</dd>
<dt><strong><code>loss_fn</code></strong> :&ensp;<code>fn</code></dt>
<dd>pytorch nn loss function</dd>
<dt><strong><code>objective</code></strong> :&ensp;<code>Objective</code></dt>
<dd>Instance of an Objective object from lib.objectives</dd>
<dt><strong><code>dataset</code></strong> :&ensp;<code>Multi_Domain_Dataset</code></dt>
<dd>Instance of a Multi_Domain_Dataset object from lib.datasets</dd>
<dt><strong><code>in_loaders_iter</code></strong> :&ensp;<code>Iterable</code></dt>
<dd>Iterable that generates tuples of dataloaders coming from each environment (usually gotten by a zip() call)</dd>
<dt><strong><code>optimizer</code></strong> :&ensp;<code>torch.optim</code></dt>
<dd>Instance of a torch.optim optimizer</dd>
<dt><strong><code>device</code></strong> :&ensp;<code>str</code></dt>
<dd>device on which the training happens</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>nn.Module</code></dt>
<dd>returns the model updated from a training step</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def train_step(model, loss_fn, objective, dataset, in_loaders_iter, optimizer, device):
    &#34;&#34;&#34; Make a single training step for a model on a dataset of the step setup with an objective

    Args:
        model (nn.Module): Model to train
        loss_fn (fn): pytorch nn loss function
        objective (Objective): Instance of an Objective object from lib.objectives
        dataset (Multi_Domain_Dataset): Instance of a Multi_Domain_Dataset object from lib.datasets
        in_loaders_iter (Iterable): Iterable that generates tuples of dataloaders coming from each environment (usually gotten by a zip() call)
        optimizer (torch.optim): Instance of a torch.optim optimizer
        device (str): device on which the training happens

    Returns:
        nn.Module: returns the model updated from a training step
    &#34;&#34;&#34;
    model.train()

    ts = torch.tensor(dataset.get_pred_time()).to(device)

    # Get next batch of training data
    # TODO: Fix that awful patch with infinite
    try:
        batch_loaders = next(in_loaders_iter)
    except StopIteration:
        _, loaders = dataset.get_train_loaders()
        in_loaders_iter = zip(*loaders)
        batch_loaders = next(in_loaders_iter)

    minibatches_device = [(x, y) for x,y in batch_loaders]

    ## Group all inputs and send to device
    all_x = torch.cat([x for x,y in minibatches_device]).to(device)
    all_y = torch.cat([y for x,y in minibatches_device]).to(device)
    all_out = []

    ## Group all inputs and get prediction
    all_out, pred = model(all_x, ts)

    # Get train loss for train environment
    train_env = [i for i, t in enumerate(ts) if i != len(ts)-1]
    env_losses = torch.zeros(len(train_env))
    for i, e in enumerate(train_env):
        env_out = all_out[e]
        env_loss = loss_fn(env_out, all_y[:,e])  
        env_losses[i] = env_loss
        objective.gather_logits_and_labels(env_out, all_y[:,e])

    # back propagate
    optimizer.zero_grad()
    objective.backward(env_losses)
    optimizer.step()

    return model</code></pre>
</details>
</dd>
<dt id="woods.lib.train_step.train_step_setup"><code class="name flex">
<span>def <span class="ident">train_step_setup</span></span>(<span>flags, training_hparams, model, objective, dataset, device)</span>
</code></dt>
<dd>
<div class="desc"><p>Train a model on a dataset of the step setup with an objective</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>flags</code></strong> :&ensp;<code>Namespace</code></dt>
<dd>training arguments</dd>
<dt><strong><code>training_hparams</code></strong> :&ensp;<code>dict</code></dt>
<dd>training related hyperparameters (lr, weight decay, batchsize, etc.)</dd>
<dt><strong><code>model</code></strong> :&ensp;<code>nn.Module</code></dt>
<dd>Model to be trained</dd>
<dt><strong><code>objective</code></strong> :&ensp;<code>Objective</code></dt>
<dd>Instance of an Objective object from lib.objectives</dd>
<dt><strong><code>dataset</code></strong> :&ensp;<code>Multi_Domain_Dataset</code></dt>
<dd>Instance of a Multi_Domain_Dataset object from lib.datasets</dd>
<dt><strong><code>device</code></strong> :&ensp;<code>str</code></dt>
<dd>device on which the training happens</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>dict</code></dt>
<dd>records from training of the model</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def train_step_setup(flags, training_hparams, model, objective, dataset, device):
    &#34;&#34;&#34; Train a model on a dataset of the step setup with an objective

    Args:
        flags (Namespace): training arguments
        training_hparams (dict): training related hyperparameters (lr, weight decay, batchsize, etc.)
        model (nn.Module): Model to be trained
        objective (Objective): Instance of an Objective object from lib.objectives
        dataset (Multi_Domain_Dataset): Instance of a Multi_Domain_Dataset object from lib.datasets
        device (str): device on which the training happens

    Returns:
        dict: records from training of the model
    &#34;&#34;&#34;

    loss_fn = nn.NLLLoss(weight=dataset.get_class_weight().to(device))
    optimizer = optim.Adam(model.parameters(), lr=training_hparams[&#39;lr&#39;], weight_decay=training_hparams[&#39;weight_decay&#39;])
    
    record = {}
    step_times = []
    
    t = utils.setup_pretty_table(flags)

    train_names, train_loaders = dataset.get_train_loaders()
    n_batches = np.sum([len(train_l) for train_l in train_loaders])

    ## Is this in the loop or not?
    train_loaders_iter = zip(*train_loaders)

    for step in range(1, dataset.N_STEPS + 1):

        ## Make training step and report accuracies and losses
        start = time.time()
        model = train_step(model, loss_fn, objective, dataset, train_loaders_iter, optimizer, device)
        step_times.append(time.time() - start)

        if step % dataset.CHECKPOINT_FREQ == 0 or (step-1) == 0:

            val_start = time.time()
            checkpoint_record = get_accuracies_step(model, loss_fn, dataset, device)
            val_time = time.time() - val_start

            record[str(step)] = checkpoint_record

            t.add_row([step] 
                    + [&#34;{:.2f} :: {:.2f}&#34;.format(record[str(step)][str(e)+&#39;_in_acc&#39;], record[str(step)][str(e)+&#39;_out_acc&#39;]) for e in dataset.get_envs()]
                    + [&#34;{:.2f}&#34;.format(np.average([record[str(step)][str(e)+&#39;_loss&#39;] for e in train_names[0]]))] 
                    + [&#34;{:.2f}&#34;.format((step*len(train_loaders)) / n_batches)]
                    + [&#34;{:.2f}&#34;.format(np.mean(step_times))]  
                    + [&#34;{:.2f}&#34;.format(val_time)])
            print(&#34;\n&#34;.join(t.get_string().splitlines()[-2:-1]))

    return model, record</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="woods.lib" href="index.html">woods.lib</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="woods.lib.train_step.get_accuracies_step" href="#woods.lib.train_step.get_accuracies_step">get_accuracies_step</a></code></li>
<li><code><a title="woods.lib.train_step.get_split_accuracy" href="#woods.lib.train_step.get_split_accuracy">get_split_accuracy</a></code></li>
<li><code><a title="woods.lib.train_step.train_step" href="#woods.lib.train_step.train_step">train_step</a></code></li>
<li><code><a title="woods.lib.train_step.train_step_setup" href="#woods.lib.train_step.train_step_setup">train_step_setup</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>
<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>woods.lib.models API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>woods.lib.models</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import math

import torch
from torch import nn
from torchvision import models

# To remove 
import matplotlib.pyplot as plt

def get_model(dataset, dataset_hparams):
    &#34;&#34;&#34;Return the dataset class with the given name.&#34;&#34;&#34;
    if dataset_hparams[&#39;model&#39;] not in globals():
        raise NotImplementedError(&#34;Dataset not found: {}&#34;.format(dataset_hparams[&#39;model&#39;]))

    model_fn = globals()[dataset_hparams[&#39;model&#39;]]

    return model_fn(dataset.get_input_size(), 
                    dataset.get_output_size(),
                    dataset_hparams)

class RNN(nn.Module):
    def __init__(self, input_size, output_size, model_hparams):
        super(RNN, self).__init__()

        # Save stuff
        self.state_size = model_hparams[&#39;state_size&#39;]
        self.hidden_depth = model_hparams[&#39;hidden_depth&#39;]
        self.hidden_width = model_hparams[&#39;hidden_width&#39;]

        ## Construct the part of the RNN in charge of the hidden state
        H_layers = []
        if self.hidden_depth == 0:
            H_layers.append( nn.Linear(input_size + self.state_size, self.state_size) )
        else:
            H_layers.append( nn.Linear(input_size + self.state_size, self.hidden_width) )
            for i in range(self.hidden_depth-1):
                H_layers.append( nn.Linear(self.hidden_width, self.hidden_width) )
            H_layers.append( nn.Linear(self.hidden_width, self.state_size) )
        
        seq_arr = []
        for i, lin in enumerate(H_layers):
            nn.init.xavier_uniform_(lin.weight)
            nn.init.zeros_(lin.bias)
            seq_arr.append(lin)
            if i != self.hidden_depth:
                seq_arr.append(nn.ReLU(True))
        self.FCH = nn.Sequential(*seq_arr)

        ## Construct the part of the model in charge of the output
        O_layers = []
        if self.hidden_depth == 0:
            O_layers.append( nn.Linear(input_size + self.state_size, output_size) )
        else:
            O_layers.append( nn.Linear(input_size + self.state_size, self.hidden_width) )
            for i in range(self.hidden_depth-1):
                O_layers.append( nn.Linear(self.hidden_width, self.hidden_width) )
            O_layers.append( nn.Linear(self.hidden_width, output_size) )
        
        seq_arr = []
        for i, lin in enumerate(O_layers):
            nn.init.xavier_uniform_(lin.weight)
            nn.init.zeros_(lin.bias)
            seq_arr.append(lin)
            if i != self.hidden_depth:
                seq_arr.append(nn.ReLU(True))
        seq_arr.append(nn.LogSoftmax(dim=1))
        self.FCO = nn.Sequential(*seq_arr)

    def forward(self, input, time_pred):

        # Setup array
        all_out = []
        pred = torch.zeros(input.shape[0], 0).to(input.device)
        hidden = self.initHidden(input.shape[0], input.device)

        # Forward propagate RNN
        for t in range(input.shape[1]):
            combined = torch.cat((input[:,t,...].view(input.shape[0],-1), hidden), 1)
            hidden = self.FCH(combined)
            out = self.FCO(combined)
            if t in time_pred:
                all_out.append(out)
                pred = torch.cat((pred, out.argmax(1, keepdim=True)), dim=1)

        return all_out, pred

    def initHidden(self, batch_size, device):
        return torch.zeros(batch_size, self.state_size).to(device)

class LSTM(nn.Module):
    def __init__(self, input_size, output_size, model_hparams):
        super(LSTM, self).__init__()

        # Save stuff
        self.state_size = model_hparams[&#39;state_size&#39;]
        self.hidden_depth = model_hparams[&#39;hidden_depth&#39;]
        self.hidden_width = model_hparams[&#39;hidden_width&#39;]
        self.recurrent_layers = model_hparams[&#39;recurrent_layers&#39;]

        # Recurrent model
        self.lstm = nn.LSTM(input_size, self.state_size, self.recurrent_layers, batch_first=True)

        # Classification model
        layers = []
        if self.hidden_depth == 0:
            layers.append( nn.Linear(self.state_size, output_size) )
        else:
            layers.append( nn.Linear(self.state_size, self.hidden_width) )
            for i in range(self.hidden_depth-1):
                layers.append( nn.Linear(self.hidden_width, self.hidden_width) )
            layers.append( nn.Linear(self.hidden_width, output_size) )
        
        seq_arr = []
        for i, lin in enumerate(layers):
            nn.init.xavier_uniform_(lin.weight)
            nn.init.zeros_(lin.bias)
            seq_arr.append(lin)
            if i != self.hidden_depth:
                seq_arr.append(nn.ReLU(True))
        seq_arr.append(nn.LogSoftmax(dim=1))
        self.classifier = nn.Sequential(*seq_arr)

    def forward(self, input, time_pred):

        # Setup array
        all_out = []
        pred = torch.zeros(input.shape[0], 0).to(input.device)
        hidden = self.initHidden(input.shape[0], input.device)

        # Forward propagate LSTM
        input = input.view(input.shape[0], input.shape[1], -1)
        out, hidden = self.lstm(input, hidden)

        # Make prediction with fully connected
        for t in time_pred:
            output = self.classifier(out[:,t,:])
            all_out.append(output)
            pred = torch.cat((pred, output.argmax(1, keepdim=True)), dim=1)
        
        return all_out, pred

    def initHidden(self, batch_size, device):
        return (torch.randn(self.recurrent_layers, batch_size, self.state_size).to(device), 
                torch.randn(self.recurrent_layers, batch_size, self.state_size).to(device))

class ATTN_LSTM(nn.Module):
    def __init__(self, input_size, output_size, model_hparams):
        super(ATTN_LSTM, self).__init__()

        self.state_size = model_hparams[&#39;state_size&#39;]
        self.hidden_depth = model_hparams[&#39;hidden_depth&#39;]
        self.hidden_width = model_hparams[&#39;hidden_width&#39;]
        self.recurrent_layers = model_hparams[&#39;recurrent_layers&#39;]

        # Recurrent model
        self.lstm = nn.LSTM(input_size, self.state_size, self.recurrent_layers, batch_first=True, dropout=0.2)

        # attention model
        layers = []
        layers.append(nn.Linear(self.state_size, self.state_size))
        seq_arr = []
        for i, lin in enumerate(layers):
            nn.init.xavier_uniform_(lin.weight)
            nn.init.zeros_(lin.bias)
            seq_arr.append(lin)
            seq_arr.append(nn.Tanh())
        self.attn = nn.Sequential(*seq_arr)
        self.sm = nn.Softmax(dim=1)
        
        # Classification model
        layers = []
        if self.hidden_depth == 0:
            layers.append( nn.Linear(self.state_size, output_size) )
        else:
            layers.append( nn.Linear(self.state_size, self.hidden_width) )
            for i in range(self.hidden_depth-1):
                layers.append( nn.Linear(self.hidden_width, self.hidden_width) )
            layers.append( nn.Linear(self.hidden_width, output_size) )
        
        seq_arr = []
        for i, lin in enumerate(layers):
            nn.init.xavier_uniform_(lin.weight)
            nn.init.zeros_(lin.bias)
            seq_arr.append(lin)
            if i != self.hidden_depth:
                seq_arr.append(nn.ReLU(True))
        seq_arr.append(nn.LogSoftmax(dim=1))
        self.classifier = nn.Sequential(*seq_arr)

    def forward(self, input, time_pred):

        # Setup array
        all_out = []
        pred = torch.zeros(input.shape[0], 0).to(input.device)
        hidden = self.initHidden(input.shape[0], input.device)

        # Forward propagate LSTM
        out, hidden = self.lstm(input, hidden)

        # attn_scores = torch.zeros_like(out)        
        # for i in range(out.shape[1]):
        #     attn_scores[:,i,:] = self.attn(out[:,i,:])
        attn_scores = self.attn(out)
        attn_scores = self.sm(attn_scores)

        out = torch.mul(out, attn_scores).sum(dim=1)

        # Make prediction with fully connected
        output = self.classifier(out)
        all_out.append(output)
        pred = torch.cat((pred, output.argmax(1, keepdim=True)), dim=1)
        
        return all_out, pred

    def initHidden(self, batch_size, device):
        return (torch.randn(self.recurrent_layers, batch_size, self.state_size).to(device), 
                torch.randn(self.recurrent_layers, batch_size, self.state_size).to(device))

class PositionalEncoding(nn.Module):
    &#34;&#34;&#34; Positional Encoding class

    https://pytorch.org/tutorials/beginner/transformer_tutorial.html
    &#34;&#34;&#34;

    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):
        super().__init__()
        self.dropout = nn.Dropout(p=dropout)

        position = torch.arange(0.0, max_len).unsqueeze(1)
        div_term = torch.exp(torch.arange(0.0, d_model, 2) * (-math.log(10000.0) / d_model))
        pe = torch.zeros(max_len, d_model)
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        self.register_buffer(&#39;pe&#39;, pe)

    def forward(self, x):
        &#34;&#34;&#34; Apply positional embedding to incoming Torch Tensor
        Args:
            x: Tensor, shape [batch_size, seq_len, embedding_dim]
        &#34;&#34;&#34;
        x = x + self.pe[:x.size(1), :].detach()
        return self.dropout(x)

class Transformer(nn.Module):
    # Do this : https://assets.amazon.science/11/88/6e046cba4241a06e536cc50584b2/gated-transformer-for-decoding-human-brain-eeg-signals.pdf
    &#34;&#34;&#34;Transformer for EEG

    Args:
        nn ([type]): [description]

    TODO:
        * Adaptive pooling to be able to use this with any input length (time)
        * Find a way to add time embedding to the input without it not working
        * Check model size to make it possible to overfit to the SEDFx dataset
    &#34;&#34;&#34;

    def __init__(self, input_size, output_size, model_hparams):
        super(Transformer, self).__init__()

        # Save stuff
        self.input_size = input_size
        self.embedding_size = model_hparams[&#39;embedding_size&#39;]

        # Define encoding layers
        self.pos_encoder = PositionalEncoding(model_hparams[&#39;embedding_size&#39;])
        enc_layer = nn.TransformerEncoderLayer(d_model=model_hparams[&#39;embedding_size&#39;], nhead=model_hparams[&#39;nheads_enc&#39;])
        # enc_layer = GatedTransformerEncoderLayer(d_model=model_hparams[&#39;embedding_size&#39;], nhead=model_hparams[&#39;nheads_enc&#39;])
        self.enc_layers = nn.TransformerEncoder(encoder_layer=enc_layer, num_layers=model_hparams[&#39;nlayers_enc&#39;])

        # Classifier
        n_conv_chs = 16
        time_conv_size = 50
        max_pool_size = 12
        pad_size = 25
        # spatial conv
        self.spatial_conv = nn.Sequential(
            nn.Conv2d(1, model_hparams[&#39;embedding_size&#39;], (1, self.input_size))
        )
        self.feature_extractor = nn.Sequential(
            # temporal conv 1
            nn.Conv2d(1, n_conv_chs, (time_conv_size, 1), padding=(pad_size, 0)),
            nn.BatchNorm2d(n_conv_chs),
            nn.GELU(),
            nn.MaxPool2d((max_pool_size, 1)),
            # temporal conv 2
            nn.Conv2d(
                n_conv_chs, n_conv_chs*2, (time_conv_size, 1),
                padding=(pad_size, 0)),
            nn.BatchNorm2d(n_conv_chs*2),
            nn.GELU(),
            nn.MaxPool2d((max_pool_size, 1)),
            # temporal conv 2
            nn.Conv2d(
                n_conv_chs*2, n_conv_chs*4, (time_conv_size, 1),
                padding=(pad_size, 0)),
            nn.BatchNorm2d(n_conv_chs*4),
            nn.GELU(),
            nn.MaxPool2d((max_pool_size, 1)) # Adaptive max pooling
        )
        self.classifier = nn.Sequential(
            nn.Linear(2048, 128),
            nn.GELU(),
            nn.Linear(128, output_size),
            nn.LogSoftmax(dim=1)
        )

    def forward(self, input, time_pred):

        # Pass through attention heads
        # out = self.embedding(input)
        out = input.unsqueeze(1)
        out = self.spatial_conv(out)
        out = out.transpose(1,3).squeeze()
        # out = self.pos_encoder(out)
        out = self.enc_layers(out)
        out = out.unsqueeze(1)
        out = self.feature_extractor(out)
        out = out.view(out.shape[0], -1)
        out = self.classifier(out)

        return [out], out.argmax(1, keepdim=True)

class CRNN(nn.Module):
    &#34;&#34;&#34; Convolutional Recurrent Neural Network
    https://github.com/HHTseng/video-classification/blob/99ebf204f0b1d737e38bc0d8b65aca128a57d7b1/ResNetCRNN/functions.py#L308
    &#34;&#34;&#34;
    def __init__(self, input_size, output_size, model_hparams):
        &#34;&#34;&#34; Initialize CRNN
        Args:
            input_size: int, size of input
            output_size: int, size of output
            model_hparams: dict, model hyperparameters
        &#34;&#34;&#34;
        super(CRNN, self).__init__()

        # Save stuff
        self.input_size = input_size
        fc_hidden1, fc_hidden2 = model_hparams[&#39;fc_hidden&#39;]
        self.CNN_embed_dim = model_hparams[&#39;CNN_embed_dim&#39;]

        # Define Resnet model
        # self.network = torchvision.models.resnet50(pretrained=True)
        resnet = models.resnet50(pretrained=True)
        self.n_outputs = resnet.fc.in_features
        modules = list(resnet.children())[:-1]      # delete the last fc layer.
        self.resnet = nn.Sequential(*modules)
        for param in self.resnet.parameters():
            param.requires_grad = False

        # Define CNN embedding
        self.cnn_fc = nn.Sequential(
            nn.Linear(resnet.fc.in_features, fc_hidden1),
            nn.BatchNorm1d(fc_hidden1, momentum=0.01),
            nn.Linear(fc_hidden1, fc_hidden2),
            nn.BatchNorm1d(fc_hidden2, momentum=0.01),
            nn.Linear(fc_hidden2, self.CNN_embed_dim),
        )

        # Define recurrent layers
        self.lstm = ATTN_LSTM(self.CNN_embed_dim, output_size, model_hparams)
        # nn.LSTM(CNN_embed_dim, model_hparams[&#39;state_size&#39;], model_hparams[&#39;recurrent_layers&#39;], batch_first=True)

    def forward(self, input, time_pred):
        &#34;&#34;&#34; Forward pass through CRNN
        Args:
            input: Tensor, shape [batch_size, seq_len, input_size]
            time_pred: Tensor, time prediction indexes
        &#34;&#34;&#34;
        # Pass through Resnet
        cnn_embed_seq = torch.zeros((input.shape[0], input.shape[1], self.CNN_embed_dim)).to(input.device)
        for t in range(input.size(1)):
            # ResNet CNN
            with torch.no_grad():
                x = self.resnet(input[:,t,...])  # ResNet
                x = x.view(x.size(0), -1)        # flatten output of conv

            # FC layers
            x = self.cnn_fc(x)

            cnn_embed_seq[:,t,:] = x

        # # swap time and sample dim such that (sample dim, time dim, CNN latent dim)
        # cnn_embed_seq = torch.stack(cnn_embed_seq, dim=1)

        # Pass through recurrent layers
        out, pred = self.lstm(cnn_embed_seq, time_pred)

        return out, pred

# class GatedTransformerEncoderLayer(nn.Module):
#     r&#34;&#34;&#34;TransformerEncoderLayer is made up of self-attn and feedforward network.
#     This standard encoder layer is based on the paper &#34;Attention Is All You Need&#34;.
#     Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
#     Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in
#     Neural Information Processing Systems, pages 6000-6010. Users may modify or implement
#     in a different way during application.

#     Args:
#         d_model: the number of expected features in the input (required).
#         nhead: the number of heads in the multiheadattention models (required).
#         dim_feedforward: the dimension of the feedforward network model (default=2048).
#         dropout: the dropout value (default=0.1).
#         activation: the activation function of intermediate layer, relu or gelu (default=relu).

#     Examples::
#         &gt;&gt;&gt; encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)
#         &gt;&gt;&gt; src = torch.rand(10, 32, 512)
#         &gt;&gt;&gt; out = encoder_layer(src)
#     &#34;&#34;&#34;

#     def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation=&#34;relu&#34;):
#         super(GatedTransformerEncoderLayer, self).__init__()
#         self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)
#         # Implementation of Feedforward model
#         self.linear1 = nn.Linear(d_model, dim_feedforward)
#         self.dropout = nn.Dropout(dropout)
#         self.linear2 = nn.Linear(dim_feedforward, d_model)

#         self.gate1 = nn.GRUCell(d_model, d_model)
#         self.gate2 = nn.GRUCell(d_model, d_model)

#         self.norm1 = nn.LayerNorm(d_model)
#         self.norm2 = nn.LayerNorm(d_model)
#         self.dropout1 = nn.Dropout(dropout)
#         self.dropout2 = nn.Dropout(dropout)

#         self.activation = nn.functional.relu

#     def __setstate__(self, state):
#         if &#39;activation&#39; not in state:
#             state[&#39;activation&#39;] = F.relu
#         super(GatedTransformerEncoderLayer, self).__setstate__(state)

#     def forward(self, src, src_mask=None, src_key_padding_mask=None):
#         r&#34;&#34;&#34;Pass the input through the encoder layer.

#         Args:
#             src: the sequence to the encoder layer (required).
#             src_mask: the mask for the src sequence (optional).
#             src_key_padding_mask: the mask for the src keys per batch (optional).

#         Shape:
#             see the docs in Transformer class.
#         &#34;&#34;&#34;
#         # First part
#         src2 = self.norm1(src)
#         src2 = self.self_attn(src2, src2, src2, attn_mask=src_mask,
#                               key_padding_mask=src_key_padding_mask)[0]
#         # src = src + self.dropout1(src2)
#         src = self.gate1(
#             rearrange(src, &#39;b n d -&gt; (b n) d&#39;),
#             rearrange(src2, &#39;b n d -&gt; (b n) d&#39;),
#         )
#         src = rearrange(src, &#39;(b n) d -&gt; b n d&#39;, b = src2.shape[0])

#         # Second part
#         src2 = self.norm2(src)
#         src2 = self.linear2(self.dropout(self.activation(self.linear1(src2))))
#         # src = src + self.dropout2(src2)
#         src = self.gate2(
#             rearrange(src, &#39;b n d -&gt; (b n) d&#39;),
#             rearrange(src2, &#39;b n d -&gt; (b n) d&#39;),
#         )
#         src = rearrange(src, &#39;(b n) d -&gt; b n d&#39;, b = src2.shape[0])

#         return src</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="woods.lib.models.get_model"><code class="name flex">
<span>def <span class="ident">get_model</span></span>(<span>dataset, dataset_hparams)</span>
</code></dt>
<dd>
<div class="desc"><p>Return the dataset class with the given name.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_model(dataset, dataset_hparams):
    &#34;&#34;&#34;Return the dataset class with the given name.&#34;&#34;&#34;
    if dataset_hparams[&#39;model&#39;] not in globals():
        raise NotImplementedError(&#34;Dataset not found: {}&#34;.format(dataset_hparams[&#39;model&#39;]))

    model_fn = globals()[dataset_hparams[&#39;model&#39;]]

    return model_fn(dataset.get_input_size(), 
                    dataset.get_output_size(),
                    dataset_hparams)</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="woods.lib.models.ATTN_LSTM"><code class="flex name class">
<span>class <span class="ident">ATTN_LSTM</span></span>
<span>(</span><span>input_size, output_size, model_hparams)</span>
</code></dt>
<dd>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ATTN_LSTM(nn.Module):
    def __init__(self, input_size, output_size, model_hparams):
        super(ATTN_LSTM, self).__init__()

        self.state_size = model_hparams[&#39;state_size&#39;]
        self.hidden_depth = model_hparams[&#39;hidden_depth&#39;]
        self.hidden_width = model_hparams[&#39;hidden_width&#39;]
        self.recurrent_layers = model_hparams[&#39;recurrent_layers&#39;]

        # Recurrent model
        self.lstm = nn.LSTM(input_size, self.state_size, self.recurrent_layers, batch_first=True, dropout=0.2)

        # attention model
        layers = []
        layers.append(nn.Linear(self.state_size, self.state_size))
        seq_arr = []
        for i, lin in enumerate(layers):
            nn.init.xavier_uniform_(lin.weight)
            nn.init.zeros_(lin.bias)
            seq_arr.append(lin)
            seq_arr.append(nn.Tanh())
        self.attn = nn.Sequential(*seq_arr)
        self.sm = nn.Softmax(dim=1)
        
        # Classification model
        layers = []
        if self.hidden_depth == 0:
            layers.append( nn.Linear(self.state_size, output_size) )
        else:
            layers.append( nn.Linear(self.state_size, self.hidden_width) )
            for i in range(self.hidden_depth-1):
                layers.append( nn.Linear(self.hidden_width, self.hidden_width) )
            layers.append( nn.Linear(self.hidden_width, output_size) )
        
        seq_arr = []
        for i, lin in enumerate(layers):
            nn.init.xavier_uniform_(lin.weight)
            nn.init.zeros_(lin.bias)
            seq_arr.append(lin)
            if i != self.hidden_depth:
                seq_arr.append(nn.ReLU(True))
        seq_arr.append(nn.LogSoftmax(dim=1))
        self.classifier = nn.Sequential(*seq_arr)

    def forward(self, input, time_pred):

        # Setup array
        all_out = []
        pred = torch.zeros(input.shape[0], 0).to(input.device)
        hidden = self.initHidden(input.shape[0], input.device)

        # Forward propagate LSTM
        out, hidden = self.lstm(input, hidden)

        # attn_scores = torch.zeros_like(out)        
        # for i in range(out.shape[1]):
        #     attn_scores[:,i,:] = self.attn(out[:,i,:])
        attn_scores = self.attn(out)
        attn_scores = self.sm(attn_scores)

        out = torch.mul(out, attn_scores).sum(dim=1)

        # Make prediction with fully connected
        output = self.classifier(out)
        all_out.append(output)
        pred = torch.cat((pred, output.argmax(1, keepdim=True)), dim=1)
        
        return all_out, pred

    def initHidden(self, batch_size, device):
        return (torch.randn(self.recurrent_layers, batch_size, self.state_size).to(device), 
                torch.randn(self.recurrent_layers, batch_size, self.state_size).to(device))</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="woods.lib.models.ATTN_LSTM.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="woods.lib.models.ATTN_LSTM.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="woods.lib.models.ATTN_LSTM.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, input, time_pred) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<div class="desc"><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, input, time_pred):

    # Setup array
    all_out = []
    pred = torch.zeros(input.shape[0], 0).to(input.device)
    hidden = self.initHidden(input.shape[0], input.device)

    # Forward propagate LSTM
    out, hidden = self.lstm(input, hidden)

    # attn_scores = torch.zeros_like(out)        
    # for i in range(out.shape[1]):
    #     attn_scores[:,i,:] = self.attn(out[:,i,:])
    attn_scores = self.attn(out)
    attn_scores = self.sm(attn_scores)

    out = torch.mul(out, attn_scores).sum(dim=1)

    # Make prediction with fully connected
    output = self.classifier(out)
    all_out.append(output)
    pred = torch.cat((pred, output.argmax(1, keepdim=True)), dim=1)
    
    return all_out, pred</code></pre>
</details>
</dd>
<dt id="woods.lib.models.ATTN_LSTM.initHidden"><code class="name flex">
<span>def <span class="ident">initHidden</span></span>(<span>self, batch_size, device)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def initHidden(self, batch_size, device):
    return (torch.randn(self.recurrent_layers, batch_size, self.state_size).to(device), 
            torch.randn(self.recurrent_layers, batch_size, self.state_size).to(device))</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="woods.lib.models.CRNN"><code class="flex name class">
<span>class <span class="ident">CRNN</span></span>
<span>(</span><span>input_size, output_size, model_hparams)</span>
</code></dt>
<dd>
<div class="desc"><p>Convolutional Recurrent Neural Network
<a href="https://github.com/HHTseng/video-classification/blob/99ebf204f0b1d737e38bc0d8b65aca128a57d7b1/ResNetCRNN/functions.py#L308">https://github.com/HHTseng/video-classification/blob/99ebf204f0b1d737e38bc0d8b65aca128a57d7b1/ResNetCRNN/functions.py#L308</a></p>
<p>Initialize CRNN</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>input_size</code></strong></dt>
<dd>int, size of input</dd>
<dt><strong><code>output_size</code></strong></dt>
<dd>int, size of output</dd>
<dt><strong><code>model_hparams</code></strong></dt>
<dd>dict, model hyperparameters</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class CRNN(nn.Module):
    &#34;&#34;&#34; Convolutional Recurrent Neural Network
    https://github.com/HHTseng/video-classification/blob/99ebf204f0b1d737e38bc0d8b65aca128a57d7b1/ResNetCRNN/functions.py#L308
    &#34;&#34;&#34;
    def __init__(self, input_size, output_size, model_hparams):
        &#34;&#34;&#34; Initialize CRNN
        Args:
            input_size: int, size of input
            output_size: int, size of output
            model_hparams: dict, model hyperparameters
        &#34;&#34;&#34;
        super(CRNN, self).__init__()

        # Save stuff
        self.input_size = input_size
        fc_hidden1, fc_hidden2 = model_hparams[&#39;fc_hidden&#39;]
        self.CNN_embed_dim = model_hparams[&#39;CNN_embed_dim&#39;]

        # Define Resnet model
        # self.network = torchvision.models.resnet50(pretrained=True)
        resnet = models.resnet50(pretrained=True)
        self.n_outputs = resnet.fc.in_features
        modules = list(resnet.children())[:-1]      # delete the last fc layer.
        self.resnet = nn.Sequential(*modules)
        for param in self.resnet.parameters():
            param.requires_grad = False

        # Define CNN embedding
        self.cnn_fc = nn.Sequential(
            nn.Linear(resnet.fc.in_features, fc_hidden1),
            nn.BatchNorm1d(fc_hidden1, momentum=0.01),
            nn.Linear(fc_hidden1, fc_hidden2),
            nn.BatchNorm1d(fc_hidden2, momentum=0.01),
            nn.Linear(fc_hidden2, self.CNN_embed_dim),
        )

        # Define recurrent layers
        self.lstm = ATTN_LSTM(self.CNN_embed_dim, output_size, model_hparams)
        # nn.LSTM(CNN_embed_dim, model_hparams[&#39;state_size&#39;], model_hparams[&#39;recurrent_layers&#39;], batch_first=True)

    def forward(self, input, time_pred):
        &#34;&#34;&#34; Forward pass through CRNN
        Args:
            input: Tensor, shape [batch_size, seq_len, input_size]
            time_pred: Tensor, time prediction indexes
        &#34;&#34;&#34;
        # Pass through Resnet
        cnn_embed_seq = torch.zeros((input.shape[0], input.shape[1], self.CNN_embed_dim)).to(input.device)
        for t in range(input.size(1)):
            # ResNet CNN
            with torch.no_grad():
                x = self.resnet(input[:,t,...])  # ResNet
                x = x.view(x.size(0), -1)        # flatten output of conv

            # FC layers
            x = self.cnn_fc(x)

            cnn_embed_seq[:,t,:] = x

        # # swap time and sample dim such that (sample dim, time dim, CNN latent dim)
        # cnn_embed_seq = torch.stack(cnn_embed_seq, dim=1)

        # Pass through recurrent layers
        out, pred = self.lstm(cnn_embed_seq, time_pred)

        return out, pred</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="woods.lib.models.CRNN.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="woods.lib.models.CRNN.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="woods.lib.models.CRNN.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, input, time_pred) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<div class="desc"><p>Forward pass through CRNN</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>input</code></strong></dt>
<dd>Tensor, shape [batch_size, seq_len, input_size]</dd>
<dt><strong><code>time_pred</code></strong></dt>
<dd>Tensor, time prediction indexes</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, input, time_pred):
    &#34;&#34;&#34; Forward pass through CRNN
    Args:
        input: Tensor, shape [batch_size, seq_len, input_size]
        time_pred: Tensor, time prediction indexes
    &#34;&#34;&#34;
    # Pass through Resnet
    cnn_embed_seq = torch.zeros((input.shape[0], input.shape[1], self.CNN_embed_dim)).to(input.device)
    for t in range(input.size(1)):
        # ResNet CNN
        with torch.no_grad():
            x = self.resnet(input[:,t,...])  # ResNet
            x = x.view(x.size(0), -1)        # flatten output of conv

        # FC layers
        x = self.cnn_fc(x)

        cnn_embed_seq[:,t,:] = x

    # # swap time and sample dim such that (sample dim, time dim, CNN latent dim)
    # cnn_embed_seq = torch.stack(cnn_embed_seq, dim=1)

    # Pass through recurrent layers
    out, pred = self.lstm(cnn_embed_seq, time_pred)

    return out, pred</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="woods.lib.models.LSTM"><code class="flex name class">
<span>class <span class="ident">LSTM</span></span>
<span>(</span><span>input_size, output_size, model_hparams)</span>
</code></dt>
<dd>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class LSTM(nn.Module):
    def __init__(self, input_size, output_size, model_hparams):
        super(LSTM, self).__init__()

        # Save stuff
        self.state_size = model_hparams[&#39;state_size&#39;]
        self.hidden_depth = model_hparams[&#39;hidden_depth&#39;]
        self.hidden_width = model_hparams[&#39;hidden_width&#39;]
        self.recurrent_layers = model_hparams[&#39;recurrent_layers&#39;]

        # Recurrent model
        self.lstm = nn.LSTM(input_size, self.state_size, self.recurrent_layers, batch_first=True)

        # Classification model
        layers = []
        if self.hidden_depth == 0:
            layers.append( nn.Linear(self.state_size, output_size) )
        else:
            layers.append( nn.Linear(self.state_size, self.hidden_width) )
            for i in range(self.hidden_depth-1):
                layers.append( nn.Linear(self.hidden_width, self.hidden_width) )
            layers.append( nn.Linear(self.hidden_width, output_size) )
        
        seq_arr = []
        for i, lin in enumerate(layers):
            nn.init.xavier_uniform_(lin.weight)
            nn.init.zeros_(lin.bias)
            seq_arr.append(lin)
            if i != self.hidden_depth:
                seq_arr.append(nn.ReLU(True))
        seq_arr.append(nn.LogSoftmax(dim=1))
        self.classifier = nn.Sequential(*seq_arr)

    def forward(self, input, time_pred):

        # Setup array
        all_out = []
        pred = torch.zeros(input.shape[0], 0).to(input.device)
        hidden = self.initHidden(input.shape[0], input.device)

        # Forward propagate LSTM
        input = input.view(input.shape[0], input.shape[1], -1)
        out, hidden = self.lstm(input, hidden)

        # Make prediction with fully connected
        for t in time_pred:
            output = self.classifier(out[:,t,:])
            all_out.append(output)
            pred = torch.cat((pred, output.argmax(1, keepdim=True)), dim=1)
        
        return all_out, pred

    def initHidden(self, batch_size, device):
        return (torch.randn(self.recurrent_layers, batch_size, self.state_size).to(device), 
                torch.randn(self.recurrent_layers, batch_size, self.state_size).to(device))</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="woods.lib.models.LSTM.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="woods.lib.models.LSTM.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="woods.lib.models.LSTM.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, input, time_pred) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<div class="desc"><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, input, time_pred):

    # Setup array
    all_out = []
    pred = torch.zeros(input.shape[0], 0).to(input.device)
    hidden = self.initHidden(input.shape[0], input.device)

    # Forward propagate LSTM
    input = input.view(input.shape[0], input.shape[1], -1)
    out, hidden = self.lstm(input, hidden)

    # Make prediction with fully connected
    for t in time_pred:
        output = self.classifier(out[:,t,:])
        all_out.append(output)
        pred = torch.cat((pred, output.argmax(1, keepdim=True)), dim=1)
    
    return all_out, pred</code></pre>
</details>
</dd>
<dt id="woods.lib.models.LSTM.initHidden"><code class="name flex">
<span>def <span class="ident">initHidden</span></span>(<span>self, batch_size, device)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def initHidden(self, batch_size, device):
    return (torch.randn(self.recurrent_layers, batch_size, self.state_size).to(device), 
            torch.randn(self.recurrent_layers, batch_size, self.state_size).to(device))</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="woods.lib.models.PositionalEncoding"><code class="flex name class">
<span>class <span class="ident">PositionalEncoding</span></span>
<span>(</span><span>d_model: int, dropout: float = 0.1, max_len: int = 5000)</span>
</code></dt>
<dd>
<div class="desc"><p>Positional Encoding class</p>
<p><a href="https://pytorch.org/tutorials/beginner/transformer_tutorial.html">https://pytorch.org/tutorials/beginner/transformer_tutorial.html</a></p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class PositionalEncoding(nn.Module):
    &#34;&#34;&#34; Positional Encoding class

    https://pytorch.org/tutorials/beginner/transformer_tutorial.html
    &#34;&#34;&#34;

    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):
        super().__init__()
        self.dropout = nn.Dropout(p=dropout)

        position = torch.arange(0.0, max_len).unsqueeze(1)
        div_term = torch.exp(torch.arange(0.0, d_model, 2) * (-math.log(10000.0) / d_model))
        pe = torch.zeros(max_len, d_model)
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        self.register_buffer(&#39;pe&#39;, pe)

    def forward(self, x):
        &#34;&#34;&#34; Apply positional embedding to incoming Torch Tensor
        Args:
            x: Tensor, shape [batch_size, seq_len, embedding_dim]
        &#34;&#34;&#34;
        x = x + self.pe[:x.size(1), :].detach()
        return self.dropout(x)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="woods.lib.models.PositionalEncoding.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="woods.lib.models.PositionalEncoding.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="woods.lib.models.PositionalEncoding.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<div class="desc"><p>Apply positional embedding to incoming Torch Tensor</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd>Tensor, shape [batch_size, seq_len, embedding_dim]</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, x):
    &#34;&#34;&#34; Apply positional embedding to incoming Torch Tensor
    Args:
        x: Tensor, shape [batch_size, seq_len, embedding_dim]
    &#34;&#34;&#34;
    x = x + self.pe[:x.size(1), :].detach()
    return self.dropout(x)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="woods.lib.models.RNN"><code class="flex name class">
<span>class <span class="ident">RNN</span></span>
<span>(</span><span>input_size, output_size, model_hparams)</span>
</code></dt>
<dd>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class RNN(nn.Module):
    def __init__(self, input_size, output_size, model_hparams):
        super(RNN, self).__init__()

        # Save stuff
        self.state_size = model_hparams[&#39;state_size&#39;]
        self.hidden_depth = model_hparams[&#39;hidden_depth&#39;]
        self.hidden_width = model_hparams[&#39;hidden_width&#39;]

        ## Construct the part of the RNN in charge of the hidden state
        H_layers = []
        if self.hidden_depth == 0:
            H_layers.append( nn.Linear(input_size + self.state_size, self.state_size) )
        else:
            H_layers.append( nn.Linear(input_size + self.state_size, self.hidden_width) )
            for i in range(self.hidden_depth-1):
                H_layers.append( nn.Linear(self.hidden_width, self.hidden_width) )
            H_layers.append( nn.Linear(self.hidden_width, self.state_size) )
        
        seq_arr = []
        for i, lin in enumerate(H_layers):
            nn.init.xavier_uniform_(lin.weight)
            nn.init.zeros_(lin.bias)
            seq_arr.append(lin)
            if i != self.hidden_depth:
                seq_arr.append(nn.ReLU(True))
        self.FCH = nn.Sequential(*seq_arr)

        ## Construct the part of the model in charge of the output
        O_layers = []
        if self.hidden_depth == 0:
            O_layers.append( nn.Linear(input_size + self.state_size, output_size) )
        else:
            O_layers.append( nn.Linear(input_size + self.state_size, self.hidden_width) )
            for i in range(self.hidden_depth-1):
                O_layers.append( nn.Linear(self.hidden_width, self.hidden_width) )
            O_layers.append( nn.Linear(self.hidden_width, output_size) )
        
        seq_arr = []
        for i, lin in enumerate(O_layers):
            nn.init.xavier_uniform_(lin.weight)
            nn.init.zeros_(lin.bias)
            seq_arr.append(lin)
            if i != self.hidden_depth:
                seq_arr.append(nn.ReLU(True))
        seq_arr.append(nn.LogSoftmax(dim=1))
        self.FCO = nn.Sequential(*seq_arr)

    def forward(self, input, time_pred):

        # Setup array
        all_out = []
        pred = torch.zeros(input.shape[0], 0).to(input.device)
        hidden = self.initHidden(input.shape[0], input.device)

        # Forward propagate RNN
        for t in range(input.shape[1]):
            combined = torch.cat((input[:,t,...].view(input.shape[0],-1), hidden), 1)
            hidden = self.FCH(combined)
            out = self.FCO(combined)
            if t in time_pred:
                all_out.append(out)
                pred = torch.cat((pred, out.argmax(1, keepdim=True)), dim=1)

        return all_out, pred

    def initHidden(self, batch_size, device):
        return torch.zeros(batch_size, self.state_size).to(device)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="woods.lib.models.RNN.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="woods.lib.models.RNN.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="woods.lib.models.RNN.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, input, time_pred) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<div class="desc"><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, input, time_pred):

    # Setup array
    all_out = []
    pred = torch.zeros(input.shape[0], 0).to(input.device)
    hidden = self.initHidden(input.shape[0], input.device)

    # Forward propagate RNN
    for t in range(input.shape[1]):
        combined = torch.cat((input[:,t,...].view(input.shape[0],-1), hidden), 1)
        hidden = self.FCH(combined)
        out = self.FCO(combined)
        if t in time_pred:
            all_out.append(out)
            pred = torch.cat((pred, out.argmax(1, keepdim=True)), dim=1)

    return all_out, pred</code></pre>
</details>
</dd>
<dt id="woods.lib.models.RNN.initHidden"><code class="name flex">
<span>def <span class="ident">initHidden</span></span>(<span>self, batch_size, device)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def initHidden(self, batch_size, device):
    return torch.zeros(batch_size, self.state_size).to(device)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="woods.lib.models.Transformer"><code class="flex name class">
<span>class <span class="ident">Transformer</span></span>
<span>(</span><span>input_size, output_size, model_hparams)</span>
</code></dt>
<dd>
<div class="desc"><p>Transformer for EEG</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>nn</code></strong> :&ensp;<code>[type]</code></dt>
<dd>[description]</dd>
</dl>
<h2 id="todo">Todo</h2>
<ul>
<li>Adaptive pooling to be able to use this with any input length (time)</li>
<li>Find a way to add time embedding to the input without it not working</li>
<li>Check model size to make it possible to overfit to the SEDFx dataset</li>
</ul>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Transformer(nn.Module):
    # Do this : https://assets.amazon.science/11/88/6e046cba4241a06e536cc50584b2/gated-transformer-for-decoding-human-brain-eeg-signals.pdf
    &#34;&#34;&#34;Transformer for EEG

    Args:
        nn ([type]): [description]

    TODO:
        * Adaptive pooling to be able to use this with any input length (time)
        * Find a way to add time embedding to the input without it not working
        * Check model size to make it possible to overfit to the SEDFx dataset
    &#34;&#34;&#34;

    def __init__(self, input_size, output_size, model_hparams):
        super(Transformer, self).__init__()

        # Save stuff
        self.input_size = input_size
        self.embedding_size = model_hparams[&#39;embedding_size&#39;]

        # Define encoding layers
        self.pos_encoder = PositionalEncoding(model_hparams[&#39;embedding_size&#39;])
        enc_layer = nn.TransformerEncoderLayer(d_model=model_hparams[&#39;embedding_size&#39;], nhead=model_hparams[&#39;nheads_enc&#39;])
        # enc_layer = GatedTransformerEncoderLayer(d_model=model_hparams[&#39;embedding_size&#39;], nhead=model_hparams[&#39;nheads_enc&#39;])
        self.enc_layers = nn.TransformerEncoder(encoder_layer=enc_layer, num_layers=model_hparams[&#39;nlayers_enc&#39;])

        # Classifier
        n_conv_chs = 16
        time_conv_size = 50
        max_pool_size = 12
        pad_size = 25
        # spatial conv
        self.spatial_conv = nn.Sequential(
            nn.Conv2d(1, model_hparams[&#39;embedding_size&#39;], (1, self.input_size))
        )
        self.feature_extractor = nn.Sequential(
            # temporal conv 1
            nn.Conv2d(1, n_conv_chs, (time_conv_size, 1), padding=(pad_size, 0)),
            nn.BatchNorm2d(n_conv_chs),
            nn.GELU(),
            nn.MaxPool2d((max_pool_size, 1)),
            # temporal conv 2
            nn.Conv2d(
                n_conv_chs, n_conv_chs*2, (time_conv_size, 1),
                padding=(pad_size, 0)),
            nn.BatchNorm2d(n_conv_chs*2),
            nn.GELU(),
            nn.MaxPool2d((max_pool_size, 1)),
            # temporal conv 2
            nn.Conv2d(
                n_conv_chs*2, n_conv_chs*4, (time_conv_size, 1),
                padding=(pad_size, 0)),
            nn.BatchNorm2d(n_conv_chs*4),
            nn.GELU(),
            nn.MaxPool2d((max_pool_size, 1)) # Adaptive max pooling
        )
        self.classifier = nn.Sequential(
            nn.Linear(2048, 128),
            nn.GELU(),
            nn.Linear(128, output_size),
            nn.LogSoftmax(dim=1)
        )

    def forward(self, input, time_pred):

        # Pass through attention heads
        # out = self.embedding(input)
        out = input.unsqueeze(1)
        out = self.spatial_conv(out)
        out = out.transpose(1,3).squeeze()
        # out = self.pos_encoder(out)
        out = self.enc_layers(out)
        out = out.unsqueeze(1)
        out = self.feature_extractor(out)
        out = out.view(out.shape[0], -1)
        out = self.classifier(out)

        return [out], out.argmax(1, keepdim=True)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="woods.lib.models.Transformer.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="woods.lib.models.Transformer.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="woods.lib.models.Transformer.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, input, time_pred) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<div class="desc"><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, input, time_pred):

    # Pass through attention heads
    # out = self.embedding(input)
    out = input.unsqueeze(1)
    out = self.spatial_conv(out)
    out = out.transpose(1,3).squeeze()
    # out = self.pos_encoder(out)
    out = self.enc_layers(out)
    out = out.unsqueeze(1)
    out = self.feature_extractor(out)
    out = out.view(out.shape[0], -1)
    out = self.classifier(out)

    return [out], out.argmax(1, keepdim=True)</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="woods.lib" href="index.html">woods.lib</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="woods.lib.models.get_model" href="#woods.lib.models.get_model">get_model</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="woods.lib.models.ATTN_LSTM" href="#woods.lib.models.ATTN_LSTM">ATTN_LSTM</a></code></h4>
<ul class="">
<li><code><a title="woods.lib.models.ATTN_LSTM.dump_patches" href="#woods.lib.models.ATTN_LSTM.dump_patches">dump_patches</a></code></li>
<li><code><a title="woods.lib.models.ATTN_LSTM.forward" href="#woods.lib.models.ATTN_LSTM.forward">forward</a></code></li>
<li><code><a title="woods.lib.models.ATTN_LSTM.initHidden" href="#woods.lib.models.ATTN_LSTM.initHidden">initHidden</a></code></li>
<li><code><a title="woods.lib.models.ATTN_LSTM.training" href="#woods.lib.models.ATTN_LSTM.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="woods.lib.models.CRNN" href="#woods.lib.models.CRNN">CRNN</a></code></h4>
<ul class="">
<li><code><a title="woods.lib.models.CRNN.dump_patches" href="#woods.lib.models.CRNN.dump_patches">dump_patches</a></code></li>
<li><code><a title="woods.lib.models.CRNN.forward" href="#woods.lib.models.CRNN.forward">forward</a></code></li>
<li><code><a title="woods.lib.models.CRNN.training" href="#woods.lib.models.CRNN.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="woods.lib.models.LSTM" href="#woods.lib.models.LSTM">LSTM</a></code></h4>
<ul class="">
<li><code><a title="woods.lib.models.LSTM.dump_patches" href="#woods.lib.models.LSTM.dump_patches">dump_patches</a></code></li>
<li><code><a title="woods.lib.models.LSTM.forward" href="#woods.lib.models.LSTM.forward">forward</a></code></li>
<li><code><a title="woods.lib.models.LSTM.initHidden" href="#woods.lib.models.LSTM.initHidden">initHidden</a></code></li>
<li><code><a title="woods.lib.models.LSTM.training" href="#woods.lib.models.LSTM.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="woods.lib.models.PositionalEncoding" href="#woods.lib.models.PositionalEncoding">PositionalEncoding</a></code></h4>
<ul class="">
<li><code><a title="woods.lib.models.PositionalEncoding.dump_patches" href="#woods.lib.models.PositionalEncoding.dump_patches">dump_patches</a></code></li>
<li><code><a title="woods.lib.models.PositionalEncoding.forward" href="#woods.lib.models.PositionalEncoding.forward">forward</a></code></li>
<li><code><a title="woods.lib.models.PositionalEncoding.training" href="#woods.lib.models.PositionalEncoding.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="woods.lib.models.RNN" href="#woods.lib.models.RNN">RNN</a></code></h4>
<ul class="">
<li><code><a title="woods.lib.models.RNN.dump_patches" href="#woods.lib.models.RNN.dump_patches">dump_patches</a></code></li>
<li><code><a title="woods.lib.models.RNN.forward" href="#woods.lib.models.RNN.forward">forward</a></code></li>
<li><code><a title="woods.lib.models.RNN.initHidden" href="#woods.lib.models.RNN.initHidden">initHidden</a></code></li>
<li><code><a title="woods.lib.models.RNN.training" href="#woods.lib.models.RNN.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="woods.lib.models.Transformer" href="#woods.lib.models.Transformer">Transformer</a></code></h4>
<ul class="">
<li><code><a title="woods.lib.models.Transformer.dump_patches" href="#woods.lib.models.Transformer.dump_patches">dump_patches</a></code></li>
<li><code><a title="woods.lib.models.Transformer.forward" href="#woods.lib.models.Transformer.forward">forward</a></code></li>
<li><code><a title="woods.lib.models.Transformer.training" href="#woods.lib.models.Transformer.training">training</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>
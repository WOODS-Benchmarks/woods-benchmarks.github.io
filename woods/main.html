<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>woods.main API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>woods.main</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import os
import json
import time
import random
import argparse
import numpy as np

import torch
from torch import nn, optim

from woods.lib import datasets
from woods.lib import models
from woods.lib import objectives
from woods.lib import hyperparams
from woods.lib import utils
from woods.lib.train_seq import train_seq_setup, get_accuracies_seq
from woods.lib.train_step import train_step_setup

#TODO:
# - add the --save option so that simple local train runs doesn&#39;t get annoyingly saved
if __name__ == &#39;__main__&#39;:

    # Device definition
    if torch.cuda.is_available():
        device = torch.device(&#34;cuda&#34;)
    else:
        device = torch.device(&#34;cpu&#34;)

    ## Args
    parser = argparse.ArgumentParser(description=&#39;Train a model on a dataset with an objective and test on a test_env&#39;)
    # Main mode
    parser.add_argument(&#39;mode&#39;, choices=[&#39;train&#39;, &#39;eval&#39;])
    # Dataset arguments
    parser.add_argument(&#39;--test_env&#39;, type=int, default = None)
    parser.add_argument(&#39;--dataset&#39;, type=str)
    parser.add_argument(&#39;--holdout_fraction&#39;, type=float, default=0.2)
    # Setup arguments
    parser.add_argument(&#39;--objective&#39;, type=str, choices=objectives.OBJECTIVES)
    # Hyperparameters arguments
    parser.add_argument(&#39;--sample_hparams&#39;, action=&#39;store_true&#39;)
    parser.add_argument(&#39;--hparams_seed&#39;, type=int, default=0)
    parser.add_argument(&#39;--trial_seed&#39;, type=int, default=0)
    # Directory arguments
    parser.add_argument(&#39;--data_path&#39;, type=str, default=&#39;~/Documents/Data/&#39;)
    parser.add_argument(&#39;--save_path&#39;, type=str, default=&#39;./results/&#39;)
    # Model evaluation arguments
    parser.add_argument(&#39;--save_model&#39;, action=&#39;store_true&#39;)
    parser.add_argument(&#39;--model_path&#39;, type=str, default=None)
    flags = parser.parse_args()

    print(&#39;Flags:&#39;)
    for k,v in sorted(vars(flags).items()):
        print(&#34;\t{}: {}&#34;.format(k, v))
    
    ## Making job ID and checking if done
    job_name = utils.get_job_name(vars(flags))

    assert isinstance(flags.test_env, int) or flags.test_env is None, &#34;Invalid test environment&#34;
    if flags.mode == &#39;train&#39;:
        assert not os.path.isfile(os.path.join(flags.save_path, job_name+&#39;.json&#39;)), &#34;\n*********************************\n*** Job Already ran and saved ***\n*********************************\n&#34;
    
    ## Getting hparams
    training_hparams = hyperparams.get_training_hparams(flags.dataset, flags.hparams_seed, flags.sample_hparams)
    objective_hparams = hyperparams.get_objective_hparams(flags.objective, flags.hparams_seed, flags.sample_hparams)
    model_hparams = hyperparams.get_model_hparams(flags.dataset, flags.hparams_seed, flags.sample_hparams)

    print(&#39;HParams:&#39;)
    for k, v in sorted(training_hparams.items()):
        print(&#39;\t{}: {}&#39;.format(k, v))
    for k, v in sorted(model_hparams.items()):
        print(&#39;\t{}: {}&#39;.format(k, v))
    for k, v in sorted(objective_hparams.items()):
        print(&#39;\t{}: {}&#39;.format(k, v))

    ## Setting dataset seed
    random.seed(0)
    np.random.seed(0)
    torch.manual_seed(0)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

    ## Make dataset
    dataset_class = datasets.get_dataset_class(flags.dataset)
    dataset = dataset_class(flags, training_hparams)
    _, in_loaders = dataset.get_train_loaders()

    # Make some checks about the dataset
    if len(in_loaders) == 1:
        assert flags.objective == &#39;ERM&#39;, &#34;Dataset has only one environment, cannot compute multi-environment penalties&#34;

    ## Setting trial seed
    random.seed(flags.trial_seed)
    np.random.seed(flags.trial_seed)
    torch.manual_seed(flags.trial_seed)

    ## Initialize a model to train
    model = models.get_model(dataset, model_hparams)
    print(&#34;Number of parameters = &#34;, sum(p.numel() for p in model.parameters() if p.requires_grad))

    ## Initialize some Objective
    objective_class = objectives.get_objective_class(flags.objective)
    objective = objective_class(model, objective_hparams)

    ## Do the thing
    model.to(device)
    if flags.mode == &#39;train&#39;:

        if dataset.get_setup() == &#39;seq&#39;:
            model, record = train_seq_setup(flags, training_hparams, model, objective, dataset, device)
        elif dataset.get_setup() == &#39;step&#39;:
            model, record = train_step_setup(flags, training_hparams, model, objective, dataset, device)
        elif dataset.get_setup() == &#39;language&#39;:
            raise NotImplementedError(&#34;Language benchmarks and models aren&#39;t implemented yet&#34;)

        if flags.save_model:
            torch.save(model.state_dict(), os.path.join(flags.save_path, job_name+&#39;.pt&#39;))

        ## Save record
        hparams = {}
        hparams.update(training_hparams)
        hparams.update(model_hparams)
        hparams.update(objective_hparams)
        record[&#39;hparams&#39;] = hparams
        record[&#39;flags&#39;] = vars(flags)
        with open(os.path.join(flags.save_path, job_name+&#39;.json&#39;), &#39;w&#39;) as f:
            json.dump(record, f)

    elif flags.mode == &#39;eval&#39;:
        
        &#34;&#34;&#34;eval mode : -- download the weights of something -- evaluate it with get_accuracy of the right setup

        Raises:
            NotImplementedError: [description]
        &#34;&#34;&#34;
        # Load the weights
        assert flags.model_path != None, &#34;You must give the model_path in order to evaluate a model&#34;
        model.load_state_dict(torch.load(os.path.join(flags.model_path)))

        # Get accuracies
        loss_fn = nn.NLLLoss(weight=dataset.get_class_weight().to(device))
        if dataset.get_setup() == &#39;seq&#39;:
            val_start = time.time()
            record = get_accuracies_seq(model, loss_fn, dataset, device)
            val_time = time.time() - val_start
        elif dataset.get_setup() == &#39;step&#39;:
            record = get_accuracies_seq(model, loss_fn, dataset, device)
        elif dataset.get_setup() == &#39;language&#39;:
            raise NotImplementedError(&#34;Language benchmarks and models aren&#39;t implemented yet&#34;)

        train_names, _ = dataset.get_train_loaders()
        t = utils.setup_pretty_table(flags)
        t.add_row([&#39;Eval&#39;] 
                + [&#34;{:.2f} :: {:.2f}&#34;.format(record[str(e)+&#39;_in_acc&#39;], record[str(e)+&#39;_out_acc&#39;]) for e in dataset.get_envs()] 
                + [&#34;{:.2f}&#34;.format(np.average([record[str(e)+&#39;_loss&#39;] for e in train_names]))] 
                + [&#39;.&#39;]
                + [&#39;.&#39;] 
                + [&#34;{:.2f}&#34;.format(val_time)])
        print(&#34;\n&#34;.join(t.get_string().splitlines()[-2:]))</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="woods" href="index.html">woods</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>